%


%Failure Analysis

@inproceedings{safont-andreu_multimodal_2023,
	address = {Phoenix, Arizona, USA},
	title = {Multimodal {Named} {Entity} {Recognition} for {Semiconductor} {Failure} {Analysis}},
	url = {https://dl.asminternational.org/istfa/proceedings/ISTFA2023/84741/16/28558},
	doi = {10.31399/asm.cp.istfa2023p0016},
	abstract = {During the activity in the Failure Analysis (FA) laboratory, all corresponding findings and conclusions are included in a series of documents known as the FA reports. They shall, in the first place, inform the requestor about the analysis results. But additionally, they shall provide information to solve similar cases. Therefore, these documents play a key role in preserving the knowledge acquired by the engineers as they become available for consultation during future works. The different information systems in FA consist of databases, file shares, wikis, or other human-readable forms. However, the heterogeneity of these databases and the large number of independent documents make it inefficient for manual consultation. In this context, this paper proposes an application of Natural Language Processing (NLP) known as Named Entity Recognition (NER), consisting of an AI-based detection of key concepts in textual data in the form of annotations. These annotations can then be used to boost search systems or other AI models.},
	language = {en},
	urldate = {2024-01-05},
	author = {Safont-Andreu, Anna and Grabner, Corinna and Burmer, Christian and Schekotihin, Konstantin},
	month = nov,
	year = {2023},
	pages = {16--22}
}

@inproceedings{grabner_recognizing_2023,
	address = {Pulau Pinang, Malaysia},
	title = {Recognizing {Named} {Entities} in {Failure} {Analysis} {Reports}},
	isbn = {9798350301649},
	url = {https://ieeexplore.ieee.org/document/10249158/},
	doi = {10.1109/IPFA58228.2023.10249158},
	abstract = {Failure Analysis (FA) is a complex laboratory activity that requires systematic documentation of all findings and conclusions obtained during an analysis to preserve knowledge acquired by engineers in this process. Current FA information systems store this data in different formats distributed across databases, file shares, wikis, or other humanreadable forms. Given a large volume of generated FA data, navigating or searching for particular information is hard since machines cannot automatically process the stored knowledge and require frequent interaction with experts. This paper investigates two applications of modern Natural Language Processing (NLP) approaches to the Named Entity Recognition (NER) task. In particular, we study the performance of two techniques, spaCy, a highly regarded Python library, and the state-of-the-art BERT Language Models (LM), pretrained on semiconductors data. Our experiments show that spaCy reached precision, recall, and F1 scores of about 23\%, whereas a BERT-based model achieved a precision of 51\%, recall of 49\%, and an F1 score of 50\% on the test corpus.},
	language = {en},
	urldate = {2024-XX-XX},
	booktitle = {2023 {IEEE} {International} {Symposium} on the {Physical} and {Failure} {Analysis} of {Integrated} {Circuits} ({IPFA})},
	publisher = {IEEE},
	author = {Grabner, Corinna and Safont-Andreu, Anna and Burmer, Christian and Hollerith, Christian and Schekotihin, Konstantin},
	month = jul,
	year = {2023},
	pages = {1--6}
}

@article{safont-andreu_artificial_2023,
	title = {Artificial {Intelligence} {Applications} in {Semiconductor} {Failure} {Analysis}},
	volume = {25},
	issn = {1537-0755},
	url = {https://dl.asminternational.org/edfa-tech/article/25/2/16/26277/Artificial-Intelligence-Applications-in},
	doi = {10.31399/asm.edfa.2023-2.p016},
	abstract = {Failure analysis is an important activity for all life-cycle stages of a semiconductor device, including design, manufacturing, and after-sales support. As the complexity of semiconductors increases, the analyses performed by an FA engineer become more and more sophisticated. AI methods can greatly simplify the work of an engineer by automating various routine tasks such as analyzing images or measurements, searching for relevant information, or scheduling operations of an FA lab. In this paper, we provide an overview of modern AI methods already used to simplify FA tasks and outline AI techniques for perspective FA applications.},
	language = {en},
	number = {2},
	urldate = {2024-01-05},
	journal = {EDFA Technical Articles},
	author = {Safont-Andreu, Anna and Schekotihin, Konstantin and Burmer, Christian and Hollerith, Christian and Ming, Xue},
	month = may,
	year = {2023},
	pages = {16--28}
}

@inproceedings{grabner_bert-based_2022,
	address = {Pasadena, California, USA},
	title = {A {BERT}-{Based} {Report} {Classification} for {Semiconductor} {Failure} {Analysis}},
	url = {https://dl.asminternational.org/istfa/proceedings/ISTFA2022/84437/28/23883},
	doi = {10.31399/asm.cp.istfa2022p0028},
	abstract = {Failure Analysis (FA) is a complex activity that requires careful and complete documentation of all findings and conclusions to preserve knowledge acquired by engineers in this process. Modern FA systems store this data in text or image formats and organize it in databases, file shares, wikis, or other human-readable forms. Given a large volume of generated FA data, navigating it or searching for particular information is hard since machines cannot process the stored knowledge automatically and require much interaction with experts.},
	language = {en},
	urldate = {2024-XX-XX},
	author = {Grabner, Corinna and Safont-Andreu, Anna and Burmer, Christian and Schekotihin, Konstantin},
	month = oct,
	year = {2022},
	pages = {28--35}
}

@inproceedings{safont-andreu_using_2021,
	address = {Phoenix, Arizona, USA},
	title = {Using {Ontologies} in {Failure} {Analysis}},
	url = {https://dl.asminternational.org/istfa/proceedings/ISTFA2021/84215/23/18243},
	doi = {10.31399/asm.cp.istfa2021p0023},
	abstract = {Fault analysis is a complex task that requires electrical engineers to perform various analyses to detect and localize a physical defect. The analysis process is very knowledge-intensive and must be precisely documented to report the issue to customers as well as to ensure the best possible reuse of the acquired experience in similar future analyses. However, writing unambiguous documentation can be complicated for many reasons, such as selecting details and results to be presented in a report, or the naming of terms and their definition. To avoid some of these issues, FA engineers must agree on a clearly defined terminology specifying methods, physical faults and their electrical signatures, tools, and relations between them. Moreover, to allow FA software systems to use this terminology, it must be stored in a format that can be interpreted similarly by both engineers and software.},
	language = {en},
	urldate = {2024-XX-XX},
	author = {Safont-Andreu, Anna and Burmer, Christian and Schekotihin, Konstantin},
	month = oct,
	year = {2021},
	pages = {23--28}
}

@inproceedings{platter_report_2021,
	address = {Phoenix, Arizona, USA},
	title = {Report {Classification} for {Semiconductor} {Failure} {Analysis}},
	url = {https://dl.asminternational.org/istfa/proceedings/ISTFA2021/84215/1/18239},
	doi = {10.31399/asm.cp.istfa2021p0001},
	abstract = {In their daily work, engineers in the Failure Analysis (FA) laboratory generate numerous documents reporting all their tasks, findings, and conclusions regarding every device they are handled. This data stores valuable knowledge for the laboratory that other experts can consult, however, the nature of it, as individual reports reporting concrete devices and their corresponding processes, makes it inefficient to consult for the human experts. In this context, the following paper proposes a Artificial Intelligence solution for the gathering of this FA knowledge stored in the numerous documents generated in the laboratory. Therefore, we have generated a dataset of FA reports along with their corresponding electrical signatures and physical failures in order to train different supervised classifiers. The results show that the models are able of capturing the patterns underlying the different jobs and predict the causes, showing slightly better results for the physical hypotheses.},
	language = {en},
	urldate = {2024-XX-XX},
	author = {Platter, Frederik and Safont-Andreu, Anna and Burmer, Christian and Schekotihin, Konstantin},
	month = oct,
	year = {2021},
	pages = {1--5}
}


%Background: AI Agents


@book{russell_artificial_2010,
	address = {Upper Saddle River},
	edition = {3rd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-604259-4},
	shorttitle = {Artificial intelligence},
	publisher = {Prentice Hall},
	author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
	year = {2010},
	keywords = {Artificial intelligence},
}


@article{wooldridge_intelligent_1995,
	title = {Intelligent agents: theory and practice},
	volume = {10},
	issn = {0269-8889, 1469-8005},
	shorttitle = {Intelligent agents},
	url = {https://www.cambridge.org/core/product/identifier/S0269888900008122/type/journal_article},
	doi = {10.1017/S0269888900008122},
	abstract = {Abstract
	The concept of an
	agent
	has become important in both artificial intelligence (AT) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary).
	Agent theory
	is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents.
	Agent architectures
	can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally,
	agent languages
	are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is
	not
	intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.},
	language = {en},
	number = {2},
	urldate = {2024-01-05},
	journal = {The Knowledge Engineering Review},
	author = {Wooldridge, Michael and Jennings, Nicholas R.},
	month = jun,
	year = {1995},
	pages = {115--152},
}

@incollection{goos_intelligent_2002,
	address = {Berlin, Heidelberg},
	title = {Intelligent {Agents}: {The} {Key} {Concepts}},
	volume = {2322},
	isbn = {978-3-540-43377-4 978-3-540-45982-8},
	shorttitle = {Intelligent {Agents}},
	url = {http://link.springer.com/10.1007/3-540-45982-0-1},
	urldate = {2024-01-05},
	booktitle = {Multi-{Agent} {Systems} and {Applications} {II}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wooldridge, Michael},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Mařík, Vladimír and Štěpánková, Olga and Krautwurmová, Hana and Luck, Michael},
	year = {2002},
	doi = {10.1007/3-540-45982-0-1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--43},
}

%Background: AI Planning

@book{ghallab_automated_2016,
	address = {New York, NY},
	title = {Automated planning and acting},
	isbn = {978-1-107-03727-4},
	url = {https://projects.laas.fr/planning/},
	abstract = {"This book is about methods and techniques that a computational agent can use for deliberative planning and acting, i.e., for deciding both which actions to perform and how to perform them, to achieve some objective. The study of deliberation has several scientific and engineering motivations"--},
	language = {eng},
	urldate = {2024-01-04},
	publisher = {Cambridge University Press},
	author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
	year = {2016},
}

@book{ghallab_automated_2004,
	address = {Amsterdam ; Boston},
	title = {Automated planning: theory and practice},
	isbn = {978-1-55860-856-6},
	shorttitle = {Automated planning},
	url = {https://projects.laas.fr/planning/aptp/index.html},
	publisher = {Elsevier/Morgan Kaufmann},
	author = {Ghallab, Malik and Nau, Dana S. and Traverso, Paolo},
	year = {2004},
	keywords = {Data processing, Production planning},
}

@book{ghallab_nau_traverso_2016, 
	place={Cambridge}, 
	title={Automated Planning and Acting}, 
	DOI={10.1017/CBO9781139583923}, 
	publisher={Cambridge University Press}, 
	author={Ghallab, Malik and Nau, Dana and Traverso, Paolo}, 
	year={2016}}
	
@article{CIMATTI200335,
	title = {Weak, strong, and strong cyclic planning via symbolic model checking},
	journal = {Artificial Intelligence},
	volume = {147},
	number = {1},
	pages = {35-84},
	year = {2003},
	note = {Planning with Uncertainty and Incomplete Information},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/S0004-3702(02)00374-0},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370202003740},
	author = {A. Cimatti and M. Pistore and M. Roveri and P. Traverso},
	keywords = {Planning in nondeterministic domains, Conditional planning, Symbolic model-checking, Binary decision diagrams},
	abstract = {Planning in nondeterministic domains yields both conceptual and practical difficulties. From the conceptual point of view, different notions of planning problems can be devised: for instance, a plan might either guarantee goal achievement, or just have some chances of success. From the practical point of view, the problem is to devise algorithms that can effectively deal with large state spaces. In this paper, we tackle planning in nondeterministic domains by addressing conceptual and practical problems. We formally characterize different planning problems, where solutions have a chance of success (“weak planning”), are guaranteed to achieve the goal (“strong planning”), or achieve the goal with iterative trial-and-error strategies (“strong cyclic planning”). In strong cyclic planning, all the executions associated with the solution plan always have a possibility of terminating and, when they do, they are guaranteed to achieve the goal. We present planning algorithms for these problem classes, and prove that they are correct and complete. We implement the algorithms in the mbp planner by using symbolic model checking techniques. We show that our approach is practical with an extensive experimental evaluation: mbp compares positively with state-of-the-art planners, both in terms of expressiveness and in terms of performance.}
}

@article{KuterNau-HTNSMC,
	author = {Kuter, Ugur and Nau, Dana and Pistore, Marco and Traverso, Paolo},
	year = {2005},
	month = {01},
	pages = {300-309},
	title = {A Hierarchical Task-Network Planner based on Symbolic Model Checking.},
	journal = {ICAPS 2005 - Proceedings of the 15th International Conference on Automated Planning and Scheduling}
}

@article{davis_commonsense_2015,
	title = {Commonsense reasoning and commonsense knowledge in artificial intelligence},
	volume = {58},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2701413},
	doi = {10.1145/2701413},
	abstract = {AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.},
	language = {en},
	number = {9},
	urldate = {2024-01-06},
	journal = {Communications of the ACM},
	author = {Davis, Ernest and Marcus, Gary},
	month = aug,
	year = {2015},
	pages = {92--103},
}

%Background: LLMs

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\35UIP9LH\\Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QZJP2ZS3\\2303.html:text/html},
}

@misc{naveed_comprehensive_2023,
	title = {A {Comprehensive} {Overview} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.06435},
	abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	month = dec,
	year = {2023},
	note = {arXiv:2307.06435 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JB5R9SDD\\Naveed et al. - 2023 - A Comprehensive Overview of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\P3JKVAIJ\\2307.html:text/html},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\35UIP9LH\\Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QZJP2ZS3\\2303.html:text/html},
}

@article{manning_human_2022,
	title = {Human {Language} {Understanding} \& {Reasoning}},
	volume = {151},
	issn = {0011-5266, 1548-6192},
	url = {https://direct.mit.edu/daed/article/151/2/127/110621/Human-Language-Understanding-amp-Reasoning},
	doi = {10.1162/daed\_a\_01905},
	abstract = {Abstract
	The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
	language = {en},
	number = {2},
	urldate = {2024-01-06},
	journal = {Daedalus},
	author = {Manning, Christopher D.},
	month = may,
	year = {2022},
	pages = {127--138},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5TATM6WL\\Manning - 2022 - Human Language Understanding & Reasoning.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VGSGUJRR\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\KGVL4KB4\\1706.html:text/html},
}

@misc{alammar_illustrated_2018,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	urldate = {2024-01-06},
	journal = {Visualizing machine learning one concept at a time.},
	author = {Alammar, Jay},
	month = jun,
	year = {2018},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QWWT43WI\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\RRHTG2K6\\1810.html:text/html},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ATIGIVF2\\Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8XS562GY\\1910.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WSSB6ABY\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GBNBRIWQ\\2005.html:text/html},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DT2B43GJ\\Liu et al. - 2021 - Pre-train, Prompt, and Predict A Systematic Surve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\F2YI69UF\\2107.html:text/html},
}

@misc{zhao_calibrate_2021,
	title = {Calibrate {Before} {Use}: {Improving} {Few}-{Shot} {Performance} of {Language} {Models}},
	shorttitle = {Calibrate {Before} {Use}},
	url = {http://arxiv.org/abs/2102.09690},
	abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0\% absolute) and reduces variance across different choices of the prompt.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
	month = jun,
	year = {2021},
	note = {arXiv:2102.09690 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BCECB3HR\\Zhao et al. - 2021 - Calibrate Before Use Improving Few-Shot Performan.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\C4WAR36M\\2102.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AL2N3ECW\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AFPBDFZJ\\2201.html:text/html},
}


@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {24824--24837},
}


@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\K4YRIKLZ\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SSX754RL\\2205.html:text/html},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UZ48XEUQ\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\N99F2K2F\\2203.html:text/html},
}

@misc{creswell_faithful_2022,
	title = {Faithful {Reasoning} {Using} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2208.14271},
	abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Creswell, Antonia and Shanahan, Murray},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14271 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IXQ2425M\\Creswell and Shanahan - 2022 - Faithful Reasoning Using Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TQ6BH6DJ\\2208.html:text/html},
}

@misc{petroni_language_2019,
	title = {Language {Models} as {Knowledge} {Bases}?},
	url = {http://arxiv.org/abs/1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv:1909.01066 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JGI3WRJR\\Petroni et al. - 2019 - Language Models as Knowledge Bases.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DN86TUXB\\1909.html:text/html},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5KUTS9JY\\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\MG4XC6B7\\2203.html:text/html},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SZZC7MVN\\Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SLCPSJR8\\1909.html:text/html},
}



%LLM-Powered Planning AI Agents: Overview Literature




@misc{yule_wang_complete_2023,
	title = {A {Complete} {Guide} to {LLMs}-based {Autonomous} {Agents} ({Part} {I})},
	url = {https://medium.com/@yulemoon/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792},
	journal = {Medium},
	author = {Yule Wang},
	month = oct,
	year = {2023},
}

@misc{wang_survey_2023,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	url = {http://arxiv.org/abs/2308.11432},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = sep,
	year = {2023},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QXL23V2P\\Wang et al. - 2023 - A Survey on Large Language Model based Autonomous .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8AF226EN\\2308.html:text/html},
}


@article{wang_survey_2024,
	title = {A survey on large language model based autonomous agents},
	volume = {18},
	issn = {2095-2228, 2095-2236},
	url = {https://link.springer.com/10.1007/s11704-024-40231-1},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Abstract
	Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
	language = {en},
	number = {6},
	urldate = {2024-04-22},
	journal = {Frontiers of Computer Science},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	month = dec,
	year = {2024},
	pages = {186345},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YRBPQDL5\\Wang et al. - 2024 - A survey on large language model based autonomous .pdf:application/pdf},
}


@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\R5P7Z9TA\\Xi et al. - 2023 - The Rise and Potential of Large Language Model Bas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\N6F43ATF\\2309.html:text/html},
}


@inproceedings{handler_taxonomy_2023,
	address = {Rome, Italy},
	title = {A {Taxonomy} for {Autonomous} {LLM}-{Powered} {Multi}-{Agent} {Architectures}:},
	isbn = {978-989-758-671-2},
	shorttitle = {A {Taxonomy} for {Autonomous} {LLM}-{Powered} {Multi}-{Agent} {Architectures}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012239100003598},
	doi = {10.5220/0012239100003598},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 15th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Händler, Thorsten},
	year = {2023},
	month={10},
	pages = {85--98},
}


@misc{zhang_igniting_2023,
	title = {Igniting {Language} {Intelligence}: {The} {Hitchhiker}'s {Guide} {From} {Chain}-of-{Thought} {Reasoning} to {Language} {Agents}},
	shorttitle = {Igniting {Language} {Intelligence}},
	url = {http://arxiv.org/abs/2311.11797},
	abstract = {Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Yao, Yao and Zhang, Aston and Tang, Xiangru and Ma, Xinbei and He, Zhiwei and Wang, Yiming and Gerstein, Mark and Wang, Rui and Liu, Gongshen and Zhao, Hai},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11797 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multiagent Systems, Computer Science - Human-Computer Interaction},
}


@inproceedings{andreas_language_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Language {Models} as {Agent} {Models}},
	url = {https://aclanthology.org/2022.findings-emnlp.423},
	doi = {10.18653/v1/2022.findings-emnlp.423},
	language = {en},
	urldate = {2024-02-03},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob},
	year = {2022},
	keywords = {PeerReviewed},
	pages = {5769--5779},
}


@misc{zhao_-depth_2023,
	title = {An {In}-depth {Survey} of {Large} {Language} {Model}-based {Artificial} {Intelligence} {Agents}},
	url = {http://arxiv.org/abs/2309.14365},
	abstract = {Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Zhao, Pengyu and Jin, Zijian and Cheng, Ning},
	month = sep,
	year = {2023},
	note = {arXiv:2309.14365 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WDWZDQCJ\\Zhao et al. - 2023 - An In-depth Survey of Large Language Model-based A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YRKVWQXH\\2309.html:text/html},
}

@misc{cheng_exploring_2024,
	title = {Exploring {Large} {Language} {Model} based {Intelligent} {Agents}: {Definitions}, {Methods}, and {Prospects}},
	shorttitle = {Exploring {Large} {Language} {Model} based {Intelligent} {Agents}},
	url = {http://arxiv.org/abs/2401.03428},
	abstract = {Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Cheng, Yuheng and Zhang, Ceyao and Zhang, Zhengwen and Meng, Xiangrui and Hong, Sirui and Li, Wenhao and Wang, Zihao and Wang, Zekai and Yin, Feng and Zhao, Junhua and He, Xiuqiang},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03428 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\S5ZV6893\\Cheng et al. - 2024 - Exploring Large Language Model based Intelligent A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GIRBNRZA\\2401.html:text/html},
}

@misc{sumers_cognitive_2023,
	title = {Cognitive {Architectures} for {Language} {Agents}},
	url = {http://arxiv.org/abs/2309.02427},
	abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02427 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8UVMAS6Q\\Sumers et al. - 2023 - Cognitive Architectures for Language Agents.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\B8FX9DQD\\2309.html:text/html},
}

@misc{liu_bolaa_2023,
	title = {{BOLAA}: {Benchmarking} and {Orchestrating} {LLM}-augmented {Autonomous} {Agents}},
	shorttitle = {{BOLAA}},
	url = {http://arxiv.org/abs/2308.05960},
	abstract = {The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, {\textbackslash}textit\{i.e.\} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at {\textbackslash}url\{https://github.com/salesforce/BOLAA\}.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05960 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\7T5HJDZX\\Liu et al. - 2023 - BOLAA Benchmarking and Orchestrating LLM-augmente.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WQNZLDQI\\2308.html:text/html},
}

@misc{weng_llm_2023,
	title = {{LLM} {Powered} {Autonomous} {Agents}},
	url = {https://lilianweng.github.io/posts/2023-06-23-agent/},
	language = {English},
	urldate = {2024-02-03},
	journal = {Lil’Log},
	author = {Weng, Lilian},
	month = jun,
	year = {2023},
}

@inproceedings{muthusamy_towards_2023,
	address = {Singapore},
	title = {Towards large language model-based personal agents in the enterprise: {Current} trends and open problems},
	shorttitle = {Towards large language model-based personal agents in the enterprise},
	url = {https://aclanthology.org/2023.findings-emnlp.461},
	doi = {10.18653/v1/2023.findings-emnlp.461},
	language = {en},
	urldate = {2024-03-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Muthusamy, Vinod and Rizk, Yara and Kate, Kiran and Venkateswaran, Praveen and Isahagian, Vatche and Gulati, Ashu and Dube, Parijat},
	year = {2023},
	month={12},
	pages = {6909--6921},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3274G3G4\\Muthusamy et al. - 2023 - Towards large language model-based personal agents.pdf:application/pdf},
}


@misc{huang_understanding_2024,
	title = {Understanding the planning of {LLM} agents: {A} survey},
	shorttitle = {Understanding the planning of {LLM} agents},
	url = {http://arxiv.org/abs/2402.02716},
	abstract = {As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Huang, Xu and Liu, Weiwen and Chen, Xiaolong and Wang, Xingmei and Wang, Hao and Lian, Defu and Wang, Yasheng and Tang, Ruiming and Chen, Enhong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\32834T5C\\Huang et al. - 2024 - Understanding the planning of LLM agents A survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UV3FED95\\2402.html:text/html},
}


@misc{mialon_augmented_2023,
	title = {Augmented {Language} {Models}: a {Survey}},
	shorttitle = {Augmented {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07842},
	abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
	urldate = {2024-02-04},
	publisher = {arXiv},
	author = {Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozière, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07842 [cs]},
	keywords = {Computer Science - Computation and Language, Preprint},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YPMYWVVQ\\Mialon et al. - 2023 - Augmented Language Models a Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IHMY7CHH\\2302.html:text/html},
}


%LLM-Powered Agents: Individual Implementations/Applications

@misc{xu_rewoo_2023,
	title = {{ReWOO}: {Decoupling} {Reasoning} from {Observations} for {Efficient} {Augmented} {Language} {Models}},
	shorttitle = {{ReWOO}},
	url = {http://arxiv.org/abs/2305.18323},
	abstract = {Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4\% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Xu, Binfeng and Peng, Zhiyuan and Lei, Bowen and Mukherjee, Subhabrata and Liu, Yuchen and Xu, Dongkuan},
	month = may,
	year = {2023},
	note = {arXiv:2305.18323 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AYVYES5X\\Xu et al. - 2023 - ReWOO Decoupling Reasoning from Observations for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\B5DPUYRG\\2305.html:text/html},
}


@misc{lu_chameleon_2023,
	title = {Chameleon: {Plug}-and-{Play} {Compositional} {Reasoning} with {Large} {Language} {Models}},
	shorttitle = {Chameleon},
	url = {http://arxiv.org/abs/2304.09842},
	abstract = {Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54\% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37\%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0\%, lifting the state of the art to 98.78\%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
	month = oct,
	year = {2023},
	note = {arXiv:2304.09842 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8AEJHWL6\\Lu et al. - 2023 - Chameleon Plug-and-Play Compositional Reasoning w.pdf:application/pdf},
}


@misc{noauthor_autogen_nodate,
	title = {{AutoGen} {UI}},
	url = {https://github.com/victordibia/autogen-ui},
	abstract = {AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve complex tasks.},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {Hugging} {Face}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = may,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YBTAAL3Y\\Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6RDSUWRA\\2303.html:text/html},
}

@misc{noauthor_httpsgithubcome2b-devawesome-ai-agents_nodate,
	title = {https://github.com/e2b-dev/awesome-ai-agents},
	url = {https://github.com/e2b-dev/awesome-ai-agents},
}

@misc{ge_openagi_2023,
	title = {{OpenAGI}: {When} {LLM} {Meets} {Domain} {Experts}},
	shorttitle = {{OpenAGI}},
	url = {http://arxiv.org/abs/2304.04370},
	abstract = {Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng},
	month = nov,
	year = {2023},
	note = {arXiv:2304.04370 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PAZ4TJSI\\Ge et al. - 2023 - OpenAGI When LLM Meets Domain Experts.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9BRZQM6Y\\2304.html:text/html},
}

@misc{dagan_dynamic_2023,
	title = {Dynamic {Planning} with a {LLM}},
	url = {http://arxiv.org/abs/2308.06391},
	abstract = {While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Dagan, Gautier and Keller, Frank and Lascarides, Alex},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06391 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6YINUMMM\\Dagan et al. - 2023 - Dynamic Planning with a LLM.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\XCGBPM6X\\2308.html:text/html},
}

@misc{noauthor_langchain_nodate,
	title = {Langchain},
	url = {https://python.langchain.com/docs/get_started/introduction},
}

@misc{noauthor_metagpt_nodate,
	title = {{MetaGPT}},
	url = {https://github.com/geekan/MetaGPT},
}

%TODO
@misc{noauthor_agentgptgpt_nodate,
	title = {{AgentGPT}/{GPT} {Assistant}},
	url = {https://agent-gpt.net/},
}

@misc{noauthor_superagi_nodate,
	title = {{SuperAGI}},
	url = {https://superagi.com/},
}

@misc{noauthor_babyagi_nodate,
	title = {{BabyAGI}},
	url = {https://github.com/yoheinakajima/babyagi},
}

@misc{nakajima_babybeeagi_2023,
	title = {{BabyBeeAGI}: {Task} {Management} and {Functionality} {Expansion} on top of {BabyAGI}},
	url = {https://yoheinakajima.com/babybeeagi-task-management-and-functionality-expansion-on-top-of-babyagi/},
	urldate = {2024-03-27},
	author = {Nakajima, Yohei},
	month = apr,
	year = {2023},
}


@misc{noauthor_autogpt_nodate,
	title = {{AutoGPT}},
	url = {https://github.com/Significant-Gravitas/AutoGPT},
}

@misc{sun_adaplanner_2023,
	title = {{AdaPlanner}: {Adaptive} {Planning} from {Feedback} with {Language} {Models}},
	shorttitle = {{AdaPlanner}},
	url = {http://arxiv.org/abs/2305.16653},
	abstract = {Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73\% and 4.11\% while utilizing 2x and 600x fewer samples, respectively.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao},
	month = may,
	year = {2023},
	note = {arXiv:2305.16653 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\HASDYEFT\\Sun et al. - 2023 - AdaPlanner Adaptive Planning from Feedback with L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3VINCZ8P\\2305.html:text/html},
}

@misc{kannan_smart-llm_2023,
	title = {{SMART}-{LLM}: {Smart} {Multi}-{Agent} {Robot} {Task} {Planning} using {Large} {Language} {Models}},
	shorttitle = {{SMART}-{LLM}},
	url = {http://arxiv.org/abs/2309.10062},
	abstract = {In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Kannan, Shyam Sundar and Venkatesh, Vishnunandan L. N. and Min, Byung-Cheol},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10062 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6FP2MVQK\\Kannan et al. - 2023 - SMART-LLM Smart Multi-Agent Robot Task Planning u.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GXTMMPSH\\2309.html:text/html},
}

@misc{hong_metagpt_2023,
	title = {{MetaGPT}: {Meta} {Programming} for {A} {Multi}-{Agent} {Collaborative} {Framework}},
	shorttitle = {{MetaGPT}},
	url = {http://arxiv.org/abs/2308.00352},
	abstract = {Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Hong, Sirui and Zhuge, Mingchen and Chen, Jonathan and Zheng, Xiawu and Cheng, Yuheng and Zhang, Ceyao and Wang, Jinlin and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin and Schmidhuber, Jürgen},
	month = nov,
	year = {2023},
	note = {arXiv:2308.00352 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IED2F6KI\\Hong et al. - 2023 - MetaGPT Meta Programming for A Multi-Agent Collab.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VLFDHC4M\\2308.html:text/html},
}

@misc{wang_voyager_2023,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = oct,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ILSW9MKS\\Wang et al. - 2023 - Voyager An Open-Ended Embodied Agent with Large L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6JSRSSU3\\2305.html:text/html},
}

@misc{ma_laser_2023,
	title = {{LASER}: {LLM} {Agent} with {State}-{Space} {Exploration} for {Web} {Navigation}},
	shorttitle = {{LASER}},
	url = {http://arxiv.org/abs/2309.08172},
	abstract = {Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to teach the model how to reason in the interactive environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible back-tracking, allowing the model to easily recover from errors. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on the WebShop task. Experimental results show that our LASER agent significantly outperforms previous methods and closes the gap with human performance on the web navigation task.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Ma, Kaixin and Zhang, Hongming and Wang, Hongwei and Pan, Xiaoman and Yu, Dong},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08172 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\D9CAH6KA\\Ma et al. - 2023 - LASER LLM Agent with State-Space Exploration for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\Q73EE6EL\\2309.html:text/html},
}

@misc{kim_language_2023,
	title = {Language {Models} can {Solve} {Computer} {Tasks}},
	url = {http://arxiv.org/abs/2303.17491},
	abstract = {Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
	month = nov,
	year = {2023},
	note = {arXiv:2303.17491 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\XQI6PCQY\\Kim et al. - 2023 - Language Models can Solve Computer Tasks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GTMGMBD4\\2303.html:text/html},
}

@misc{lin_swiftsage_2023,
	title = {{SwiftSage}: {A} {Generative} {Agent} with {Fast} and {Slow} {Thinking} for {Complex} {Interactive} {Tasks}},
	shorttitle = {{SwiftSage}},
	url = {http://arxiv.org/abs/2305.17390},
	abstract = {We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex real-world tasks.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Ammanabrolu, Prithviraj and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
	month = may,
	year = {2023},
	note = {arXiv:2305.17390 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BJI4LJ2Q\\Lin et al. - 2023 - SwiftSage A Generative Agent with Fast and Slow T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UENWJZK7\\2305.html:text/html},
}

@misc{li_camel_2023,
	title = {{CAMEL}: {Communicative} {Agents} for "{Mind}" {Exploration} of {Large} {Language} {Model} {Society}},
	shorttitle = {{CAMEL}},
	url = {http://arxiv.org/abs/2303.17760},
	abstract = {The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
	month = nov,
	year = {2023},
	note = {arXiv:2303.17760 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AVAV23D7\\Li et al. - 2023 - CAMEL Communicative Agents for Mind Exploration.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WDEANZRN\\2303.html:text/html},
}

@article{singh_progprompt_2023,
	title = {{ProgPrompt}: program generation for situated robot task planning using large language models},
	issn = {0929-5593, 1573-7527},
	shorttitle = {{ProgPrompt}},
	url = {https://link.springer.com/10.1007/s10514-023-10135-3},
	doi = {10.1007/s10514-023-10135-3},
	abstract = {Abstract
	
	Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example  that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at
	progprompt.github.io},
	language = {en},
	urldate = {2023-11-26},
	journal = {Autonomous Robots},
	author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	month = aug,
	year = {2023},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PPSSG6QE\\Singh et al. - 2023 - ProgPrompt program generation for situated robot .pdf:application/pdf},
}

@misc{xie_translating_2023,
	title = {Translating {Natural} {Language} to {Planning} {Goals} with {Large}-{Language} {Models}},
	url = {http://arxiv.org/abs/2302.05128},
	abstract = {Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals specified in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domain-independent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.},
	urldate = {2023-11-26},
	publisher = {arXiv},
	author = {Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics, PDDL},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6D6RQDFD\\Xie et al. - 2023 - Translating Natural Language to Planning Goals wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3PIZAI3P\\2302.html:text/html},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4W48AK97\\Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5T9UWL5D\\2210.html:text/html},
}

@misc{noauthor_huggingface_nodate,
	title = {{HuggingFace} {Transformers} {Agents}},
	url = {https://huggingface.co/docs/transformers/transformers_agents},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\W8V7ZZA7\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3H74939Z\\2308.html:text/html},
}

@misc{ahn_as_2022,
	title = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
	shorttitle = {Do {As} {I} {Can}, {Not} {As} {I} {Say}},
	url = {http://arxiv.org/abs/2204.01691},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
	month = aug,
	year = {2022},
	note = {arXiv:2204.01691 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\T3NR6VMK\\Ahn et al. - 2022 - Do As I Can, Not As I Say Grounding Language in R.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GZXKMU22\\2204.html:text/html},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\CXYZ5JKI\\Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforceme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VKC3M2WB\\2303.html:text/html},
}

@misc{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {http://arxiv.org/abs/2201.07207},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = mar,
	year = {2022},
	note = {arXiv:2201.07207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UPQKMVUY\\Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IZUQVLAQ\\2201.html:text/html},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05608 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\XMLZ4MVA\\Huang et al. - 2022 - Inner Monologue Embodied Reasoning through Planni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5NBDUVFV\\2207.html:text/html},
}

@misc{capitanelli_framework_2023,
	title = {A {Framework} for {Neurosymbolic} {Robot} {Action} {Planning} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.00438},
	abstract = {Symbolic task planning is a widely used approach to enforce robot autonomy due to its ease of understanding and deployment. However, symbolic task planning is difficult to scale in real-world when frequent re-planning is needed, for example, due to human-robot interactions or unforeseen events. Plan length and planning time can hinder the robot's efficiency and negatively affect the overall human-robot interaction's fluency. We present a framework, Teriyaki, designed to bridge the gap between symbolic task planning and machine learning approaches, by training Large Language Models (LLMs), namely GPT-3, into neurosymbolic task planners compatible with the Planning Domain Definition Language (PDDL). Potential benefits include: (i) better scalability in so far as the planning domain complexity increases, since LLMs' response time linearly scales with the combined length of the input and the output, instead of super-linearly as in the case of symbolic task planners, and (ii) the ability to synthesize a plan action-by-action instead of end-to-end, and to make each action available for execution as soon as it is generated, which in turn enables concurrent planning and execution. In the past year, significant efforts have been devoted by the research community to evaluate the overall cognitive abilities of LLMs, with alternate successes. Instead, with Teriyaki we aim to providing an overall planning performance comparable to traditional planners in specific planning domains, while leveraging LLMs capabilities in other metrics which are used to build a look-ahead predictive planning model. Preliminary results in selected domains show that our method can: (i) solve 95.5\% of problems in a test data set of 1000 samples; (ii) produce plans up to 13.5\% shorter than a traditional symbolic planner; (iii) reduce average overall waiting times for a plan availability by up to 61.4\%.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Capitanelli, Alessio and Mastrogiovanni, Fulvio},
	month = dec,
	year = {2023},
	note = {arXiv:2303.00438 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, I.2.6, I.2.8, I.2.9},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\HHBR79IK\\Capitanelli and Mastrogiovanni - 2023 - A Framework for Neurosymbolic Robot Action Plannin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\II9J7ZT3\\2303.html:text/html},
}

@misc{liu_reason_2023,
	title = {Reason for {Future}, {Act} for {Now}: {A} {Principled} {Framework} for {Autonomous} {LLM} {Agents} with {Provable} {Sample} {Efficiency}},
	shorttitle = {Reason for {Future}, {Act} for {Now}},
	url = {http://arxiv.org/abs/2309.17382},
	abstract = {Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" ({\textbackslash}texttt\{RAFA\}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an "in-context" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a \${\textbackslash}sqrt\{T\}\$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Liu, Zhihan and Hu, Hao and Zhang, Shenao and Guo, Hongyi and Ke, Shuqi and Liu, Boyi and Wang, Zhaoran},
	month = oct,
	year = {2023},
	note = {arXiv:2309.17382 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PUGRU5FE\\Liu et al. - 2023 - Reason for Future, Act for Now A Principled Frame.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\RWQD6SAF\\2309.html:text/html},
}

@misc{raman_cape_2023,
	title = {{CAPE}: {Corrective} {Actions} from {Precondition} {Errors} using {Large} {Language} {Models}},
	shorttitle = {{CAPE}},
	url = {http://arxiv.org/abs/2211.09935},
	abstract = {Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89\% to 49.63\% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49\% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Raman, Shreyas Sundara and Cohen, Vanya and Paulius, David and Idrees, Ifrah and Rosen, Eric and Mooney, Ray and Tellex, Stefanie},
	month = oct,
	year = {2023},
	note = {arXiv:2211.09935 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, I.2.8, 68T20, 68T50, I.2.2, I.2.4, I.2.7},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AW7EWM6S\\Raman et al. - 2023 - CAPE Corrective Actions from Precondition Errors .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\L3MRWFRJ\\2211.html:text/html},
}

@misc{gur_real-world_2023,
	title = {A {Real}-{World} {WebAgent} with {Planning}, {Long} {Context} {Understanding}, and {Program} {Synthesis}},
	url = {http://arxiv.org/abs/2307.12856},
	abstract = {Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7\% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
	month = oct,
	year = {2023},
	note = {arXiv:2307.12856 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8RURG5YH\\Gur et al. - 2023 - A Real-World WebAgent with Planning, Long Context .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\XXBGKWXB\\2307.html:text/html},
}

@misc{tang_towards_2023,
	title = {Towards {CausalGPT}: {A} {Multi}-{Agent} {Approach} for {Faithful} {Knowledge} {Reasoning} via {Promoting} {Causal} {Consistency} in {LLMs}},
	shorttitle = {Towards {CausalGPT}},
	url = {http://arxiv.org/abs/2308.11914},
	abstract = {Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the {\textbackslash}textit\{evaluator\} agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Tang, Ziyi and Wang, Ruilin and Chen, Weixing and Wang, Keze and Liu, Yang and Chen, Tianshui and Lin, Liang},
	month = sep,
	year = {2023},
	note = {arXiv:2308.11914 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UBEH92I6\\Tang et al. - 2023 - Towards CausalGPT A Multi-Agent Approach for Fait.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SI8AGECH\\2308.html:text/html},
}

@misc{zheng_jarvis_2022,
	title = {{JARVIS}: {A} {Neuro}-{Symbolic} {Commonsense} {Reasoning} {Framework} for {Conversational} {Embodied} {Agents}},
	shorttitle = {{JARVIS}},
	url = {http://arxiv.org/abs/2208.13266},
	abstract = {Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1{\textbackslash}\% to 15.8{\textbackslash}\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Zheng, Kaizhi and Zhou, Kaiwen and Gu, Jing and Fan, Yue and Wang, Jialu and Di, Zonglin and He, Xuehai and Wang, Xin Eric},
	month = sep,
	year = {2022},
	note = {arXiv:2208.13266 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WLDEUCVK\\Zheng et al. - 2022 - JARVIS A Neuro-Symbolic Commonsense Reasoning Fra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\243DCJ4S\\2208.html:text/html},
}

@misc{li_human-centered_2023,
	title = {Human-{Centered} {Planning}},
	url = {http://arxiv.org/abs/2311.04403},
	abstract = {LLMs have recently made impressive inroads on tasks whose output is structured, such as coding, robotic planning and querying databases. The vision of creating AI-powered personal assistants also involves creating structured outputs, such as a plan for one's day, or for an overseas trip. Here, since the plan is executed by a human, the output doesn't have to satisfy strict syntactic constraints. A useful assistant should also be able to incorporate vague constraints specified by the user in natural language. This makes LLMs an attractive option for planning. We consider the problem of planning one's day. We develop an LLM-based planner (LLMPlan) extended with the ability to self-reflect on its output and a symbolic planner (SymPlan) with the ability to translate text constraints into a symbolic representation. Despite no formal specification of constraints, we find that LLMPlan performs explicit constraint satisfaction akin to the traditional symbolic planners on average (2\% performance difference), while retaining the reasoning of implicit requirements. Consequently, LLM-based planners outperform their symbolic counterparts in user satisfaction (70.5\% vs. 40.4\%) during interactive evaluation with 40 users.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Li, Yuliang and Kamra, Nitin and Desai, Ruta and Halevy, Alon},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04403 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GCN8FBDH\\Li et al. - 2023 - Human-Centered Planning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GLQKNRUB\\2311.html:text/html},
}

@misc{silver_generalized_2023,
	title = {Generalized {Planning} in {PDDL} {Domains} with {Pretrained} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.11014},
	abstract = {Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B. and Kaelbling, Leslie Pack and Katz, Michael},
	month = dec,
	year = {2023},
	note = {arXiv:2305.11014 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\C4XLR7QC\\Silver et al. - 2023 - Generalized Planning in PDDL Domains with Pretrain.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\V8BUSACH\\2305.html:text/html},
}

@misc{guan_leveraging_2023,
	title = {Leveraging {Pre}-trained {Large} {Language} {Models} to {Construct} and {Utilize} {World} {Models} for {Model}-based {Task} {Planning}},
	url = {http://arxiv.org/abs/2305.14909},
	abstract = {There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2305.14909 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JCXZV6KE\\Guan et al. - 2023 - Leveraging Pre-trained Large Language Models to Co.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GPH78CFT\\2305.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4IH29PEM\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\96B3TUA5\\2304.html:text/html},
}

@misc{chia_contrastive_2023,
	title = {Contrastive {Chain}-of-{Thought} {Prompting}},
	url = {http://arxiv.org/abs/2311.09277},
	abstract = {Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Chia, Yew Ken and Chen, Guizhen and Tuan, Luu Anh and Poria, Soujanya and Bing, Lidong},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09277 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5Y3CDT8N\\Chia et al. - 2023 - Contrastive Chain-of-Thought Prompting.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\G6C3H895\\2311.html:text/html},
}

@misc{wang_jarvis-1_2023,
	title = {{JARVIS}-1: {Open}-{World} {Multi}-task {Agents} with {Memory}-{Augmented} {Multimodal} {Language} {Models}},
	shorttitle = {{JARVIS}-1},
	url = {http://arxiv.org/abs/2311.05997},
	abstract = {Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., "chopping trees" to long-horizon tasks, e.g., "obtaining a diamond pickaxe". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of \${\textbackslash}texttt\{ObtainDiamondPickaxe\}\$, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available at https://craftjarvis.org/JARVIS-1},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Jin, Yonggang and Hou, Jinbing and Zhang, Bowei and Lin, Haowei and He, Zhaofeng and Zheng, Zilong and Yang, Yaodong and Ma, Xiaojian and Liang, Yitao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05997 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\Y9EY8358\\Wang et al. - 2023 - JARVIS-1 Open-World Multi-task Agents with Memory.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PSU5LQWR\\2311.html:text/html},
}



@misc{zhou_agents_2023,
	title = {Agents: {An} {Open}-source {Framework} for {Autonomous} {Language} {Agents}},
	shorttitle = {Agents},
	url = {http://arxiv.org/abs/2309.07870},
	abstract = {Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Li, Long and Wu, Jialong and Wang, Tiannan and Qiu, Shi and Zhang, Jintian and Chen, Jing and Wu, Ruipu and Wang, Shuai and Zhu, Shiding and Chen, Jiyu and Zhang, Wentao and Tang, Xiangru and Zhang, Ningyu and Chen, Huajun and Cui, Peng and Sachan, Mrinmaya},
	month = dec,
	year = {2023},
	note = {arXiv:2309.07870 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BUYDVGJI\\Zhou et al. - 2023 - Agents An Open-source Framework for Autonomous La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\G9KXXFKF\\2309.html:text/html},
}

@misc{wang_describe_2023,
	title = {Describe, {Explain}, {Plan} and {Select}: {Interactive} {Planning} with {Large} {Language} {Models} {Enables} {Open}-{World} {Multi}-{Task} {Agents}},
	shorttitle = {Describe, {Explain}, {Plan} and {Select}},
	url = {http://arxiv.org/abs/2302.01560},
	abstract = {We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "\${\textbackslash}underline\{D\}\$escribe, \${\textbackslash}underline\{E\}\$xplain, \${\textbackslash}underline\{P\}\$lan and \${\textbackslash}underline\{S\}\$elect" (\${\textbackslash}textbf\{DEPS\}\$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated \${\textbackslash}textit\{plan\}\$ by integrating \${\textbackslash}textit\{description\}\$ of the plan execution process and providing self-\${\textbackslash}textit\{explanation\}\$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal \${\textbackslash}textit\{selector\}\$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \${\textbackslash}texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Wang, Zihao and Cai, Shaofei and Chen, Guanzhou and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
	month = oct,
	year = {2023},
	note = {arXiv:2302.01560 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\25A7MH2L\\Wang et al. - 2023 - Describe, Explain, Plan and Select Interactive Pl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3QZZFVM2\\2302.html:text/html},
}

@misc{ruan_tptu_2023,
	title = {{TPTU}: {Large} {Language} {Model}-based {AI} {Agents} for {Task} {Planning} and {Tool} {Usage}},
	shorttitle = {{TPTU}},
	url = {http://arxiv.org/abs/2308.03427},
	abstract = {With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Li, Ziyue and Zeng, Xingyu and Zhao, Rui},
	month = nov,
	year = {2023},
	note = {arXiv:2308.03427 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8J43P243\\Ruan et al. - 2023 - TPTU Large Language Model-based AI Agents for Tas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\HGUS6FG5\\2308.html:text/html},
}

@misc{zhou_language_2023,
	title = {Language {Agent} {Tree} {Search} {Unifies} {Reasoning} {Acting} and {Planning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2310.04406},
	abstract = {While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
	month = dec,
	year = {2023},
	note = {arXiv:2310.04406 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4GRPLYN7\\Zhou et al. - 2023 - Language Agent Tree Search Unifies Reasoning Actin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\I2Q2BUWQ\\2310.html:text/html},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\S9KIQDXG\\Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\2FQIWQ4X\\2302.html:text/html},
}

@misc{liu_llmp_2023,
	title = {{LLM}+{P}: {Empowering} {Large} {Language} {Models} with {Optimal} {Planning} {Proficiency}},
	shorttitle = {{LLM}+{P}},
	url = {http://arxiv.org/abs/2304.11477},
	abstract = {Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.{\textbackslash}footnote\{\vphantom{\}}The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
	month = sep,
	year = {2023},
	note = {arXiv:2304.11477 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FD6JAQT2\\Liu et al. - 2023 - LLM+P Empowering Large Language Models with Optim.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\RXRJQ4ME\\2304.html:text/html},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-{Solve} {Prompting}: {Improving} {Zero}-{Shot} {Chain}-of-{Thought} {Reasoning} by {Large} {Language} {Models}},
	shorttitle = {Plan-and-{Solve} {Prompting}},
	url = {http://arxiv.org/abs/2305.04091},
	abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	month = may,
	year = {2023},
	note = {arXiv:2305.04091 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\93R745IS\\Wang et al. - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chai.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5Q5DWUAP\\2305.html:text/html},
}

@misc{hao_reasoning_2023,
	title = {Reasoning with {Language} {Model} is {Planning} with {World} {Model}},
	url = {http://arxiv.org/abs/2305.14992},
	abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal \${\textbackslash}textit\{world model\}\$ to predict the world \${\textbackslash}textit\{state\}\$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, \${\textbackslash}underline\{R\}\$easoning vi\${\textbackslash}underline\{a\}\$ \${\textbackslash}underline\{P\}\$lanning \${\textbackslash}textbf\{(RAP)\}\$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration \${\textbackslash}textit\{vs.\}\$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33\% relative improvement in a plan generation setting.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14992 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TYB4H7ZT\\Hao et al. - 2023 - Reasoning with Language Model is Planning with Wor.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\USQE3SJZ\\2305.html:text/html},
}

@misc{zhen_robot_2023,
	title = {Robot {Task} {Planning} {Based} on {Large} {Language} {Model} {Representing} {Knowledge} with {Directed} {Graph} {Structures}},
	url = {http://arxiv.org/abs/2306.05171},
	abstract = {Traditional robot task planning methods face challenges when dealing with highly unstructured environments and complex tasks. We propose a task planning method that combines human expertise with an LLM and have designed an LLM prompt template, Think\_Net\_Prompt, with stronger expressive power to represent structured professional knowledge. We further propose a method to progressively decompose tasks and generate a task tree to reduce the planning volume for each task, and we have designed a strategy to decouple robot task planning. By dividing different planning entities and separating the task from the actual machine binding process, the task planning process becomes more flexible. Research results show that our method performs well in handling specified code formats, understanding the relationship between tasks and subtasks, and extracting parameters from text descriptions. However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly. Improving the precision of task description and cognitive structure can bring certain improvements. https://github.com/NOMIzy/Think\_Net\_Prompt},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Zhen, Yue and Bi, Sheng and Xing-tong, Lu and Wei-qin, Pan and Hai-peng, Shi and Zi-rui, Chen and Yi-shu, Fang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IBWIAZNJ\\Zhen et al. - 2023 - Robot Task Planning Based on Large Language Model .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WDGA57XA\\2306.html:text/html},
}

@misc{prasad_adapt_2023,
	title = {{ADaPT}: {As}-{Needed} {Decomposition} and {Planning} with {Language} {Models}},
	shorttitle = {{ADaPT}},
	url = {http://arxiv.org/abs/2311.05772},
	abstract = {Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3\% higher in ALFWorld, 27\% in WebShop, and 33\% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Prasad, Archiki and Koller, Alexander and Hartmann, Mareike and Clark, Peter and Sabharwal, Ashish and Bansal, Mohit and Khot, Tushar},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05772 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YY272QMS\\Prasad et al. - 2023 - ADaPT As-Needed Decomposition and Planning with L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\H9VHQUUW\\2311.html:text/html},
}

@misc{qin_toolllm_2023,
	title = {{ToolLLM}: {Facilitating} {Large} {Language} {Models} to {Master} 16000+ {Real}-world {APIs}},
	shorttitle = {{ToolLLM}},
	url = {http://arxiv.org/abs/2307.16789},
	abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2023},
	note = {arXiv:2307.16789 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3KKRKQEG\\Qin et al. - 2023 - ToolLLM Facilitating Large Language Models to Mas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VMWRLV34\\2307.html:text/html},
}

@misc{ruan_tptu_2023-1,
	title = {{TPTU}: {Large} {Language} {Model}-based {AI} {Agents} for {Task} {Planning} and {Tool} {Usage}},
	shorttitle = {{TPTU}},
	url = {http://arxiv.org/abs/2308.03427},
	abstract = {With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Li, Ziyue and Zeng, Xingyu and Zhao, Rui},
	month = nov,
	year = {2023},
	note = {arXiv:2308.03427 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6A37DPLR\\Ruan et al. - 2023 - TPTU Large Language Model-based AI Agents for Tas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DDHIF4I2\\2308.html:text/html},
}

@misc{xie_openagents_2023,
	title = {{OpenAgents}: {An} {Open} {Platform} for {Language} {Agents} in the {Wild}},
	shorttitle = {{OpenAgents}},
	url = {http://arxiv.org/abs/2310.10634},
	abstract = {Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Xie, Tianbao and Zhou, Fan and Cheng, Zhoujun and Shi, Peng and Weng, Luoxuan and Liu, Yitao and Hua, Toh Jing and Zhao, Junning and Liu, Qian and Liu, Che and Liu, Leo Z. and Xu, Yiheng and Su, Hongjin and Shin, Dongchan and Xiong, Caiming and Yu, Tao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10634 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\I3N6JWKU\\Xie et al. - 2023 - OpenAgents An Open Platform for Language Agents i.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\J7XC82QU\\2310.html:text/html},
}

@misc{besta_graph_2023,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = nov,
	year = {2023},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WTEK7WT7\\Besta et al. - 2023 - Graph of Thoughts Solving Elaborate Problems with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\98TADDM6\\2308.html:text/html},
}

@misc{song_llm-planner_2023,
	title = {{LLM}-{Planner}: {Few}-{Shot} {Grounded} {Planning} for {Embodied} {Agents} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Planner}},
	url = {http://arxiv.org/abs/2212.04088},
	abstract = {This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5\% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2212.04088 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SP935CTA\\Song et al. - 2023 - LLM-Planner Few-Shot Grounded Planning for Embodi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ANNCAP28\\2212.html:text/html},
}

@misc{logeswaran_few-shot_2022,
	title = {Few-shot {Subgoal} {Planning} with {Language} {Models}},
	url = {http://arxiv.org/abs/2205.14288},
	abstract = {Pre-trained large language models have shown successful progress in many language understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained language models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Logeswaran, Lajanugen and Fu, Yao and Lee, Moontae and Lee, Honglak},
	month = may,
	year = {2022},
	note = {arXiv:2205.14288 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5VG4Q96S\\Logeswaran et al. - 2022 - Few-shot Subgoal Planning with Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PWQGGGSH\\2205.html:text/html},
}

@misc{mao_language_2023,
	title = {A {Language} {Agent} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2311.10813},
	abstract = {Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods. Code will be released.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Mao, Jiageng and Ye, Junjie and Qian, Yuxi and Pavone, Marco and Wang, Yue},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10813 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BWQIQMH2\\Mao et al. - 2023 - A Language Agent for Autonomous Driving.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\NCLSWXFC\\2311.html:text/html},
}

@misc{du_anytool_2024,
	title = {{AnyTool}: {Self}-{Reflective}, {Hierarchical} {Agents} for {Large}-{Scale} {API} {Calls}},
	shorttitle = {{AnyTool}},
	url = {http://arxiv.org/abs/2402.04253},
	abstract = {We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4\% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Du, Yu and Wei, Fangyun and Zhang, Hongyang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04253 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8N7PCGGU\\Du et al. - 2024 - AnyTool Self-Reflective, Hierarchical Agents for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ZFK67QAX\\2402.html:text/html},
}

@misc{zhu_plangpt_2024,
	title = {{PlanGPT}: {Enhancing} {Urban} {Planning} with {Tailored} {Language} {Model} and {Efficient} {Retrieval}},
	shorttitle = {{PlanGPT}},
	url = {http://arxiv.org/abs/2402.19273},
	abstract = {In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zhu, He and Zhang, Wenjia and Huang, Nuoxian and Li, Boyang and Niu, Luyao and Fan, Zipei and Lun, Tianle and Tao, Yicheng and Su, Junyou and Gong, Zhaoya and Fang, Chenyu and Liu, Xing},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19273 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JE4I5VA4\\Zhu et al. - 2024 - PlanGPT Enhancing Urban Planning with Tailored La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\K8D59Q4G\\2402.html:text/html},
}

@misc{zhao_empowering_2024,
	title = {Empowering {Large} {Language} {Model} {Agents} through {Action} {Learning}},
	url = {http://arxiv.org/abs/2402.15809},
	abstract = {Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zhao, Haiteng and Ma, Chang and Wang, Guoyin and Su, Jing and Kong, Lingpeng and Xu, Jingjing and Deng, Zhi-Hong and Yang, Hongxia},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15809 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4Y589FU9\\Zhao et al. - 2024 - Empowering Large Language Model Agents through Act.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VNS33LI5\\2402.html:text/html},
}

@misc{paul_sequential_2023,
	title = {Sequential {Planning} in {Large} {Partially} {Observable} {Environments} guided by {LLMs}},
	url = {http://arxiv.org/abs/2312.07368},
	abstract = {Sequential planning in large state space and action space quickly becomes intractable due to combinatorial explosion of the search space. Heuristic methods, like monte-carlo tree search, though effective for large state space, but struggle if action space is large. Pure reinforcement learning methods, relying only on reward signals, needs prohibitively large interactions with the environment to device a viable plan. If the state space, observations and actions can be represented in natural language then Large Language models (LLM) can be used to generate action plans. Recently several such goal-directed agents like Reflexion, CLIN, SayCan were able to surpass the performance of other state-of-the-art methods with minimum or no task specific training. But they still struggle with exploration and get stuck in local optima. Their planning capabilities are limited by the limited reasoning capability of the foundational LLMs on text data. We propose a hybrid agent "neoplanner", that synergizes both state space search with queries to foundational LLM to get the best action plan. The reward signals are quantitatively used to drive the search. A balance of exploration and exploitation is maintained by maximizing upper confidence bounds of values of states. In places where random exploration is needed, the LLM is queried to generate an action plan. Learnings from each trial are stored as entity relationships in text format. Those are used in future queries to the LLM for continual improvement. Experiments in the Scienceworld environment reveals a 124\% improvement from the current best method in terms of average reward gained across multiple tasks.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Paul, Swarna Kamal},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07368 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UJ244FU5\\Paul - 2023 - Sequential Planning in Large Partially Observable .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9N8XPMT9\\2312.html:text/html},
}

@misc{ouyang_autoplan_2023,
	title = {{AutoPlan}: {Automatic} {Planning} of {Interactive} {Decision}-{Making} {Tasks} {With} {Large} {Language} {Models}},
	shorttitle = {{AutoPlan}},
	url = {http://arxiv.org/abs/2305.15064},
	abstract = {Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8\% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Ouyang, Siqi and Li, Lei},
	month = oct,
	year = {2023},
	note = {arXiv:2305.15064 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\KU2WMR5M\\Ouyang and Li - 2023 - AutoPlan Automatic Planning of Interactive Decisi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WVWE89II\\2305.html:text/html},
}

@misc{qiao_taskweaver_2023,
	title = {{TaskWeaver}: {A} {Code}-{First} {Agent} {Framework}},
	shorttitle = {{TaskWeaver}},
	url = {http://arxiv.org/abs/2311.17541},
	abstract = {Large Language Models (LLMs) have shown impressive abilities in natural language understanding and generation, leading to their use in applications such as chatbots and virtual assistants. However, existing LLM frameworks face limitations in handling domain-specific data analytics tasks with rich data structures. Moreover, they struggle with flexibility to meet diverse user requirements. To address these issues, TaskWeaver is proposed as a code-first framework for building LLM-powered autonomous agents. It converts user requests into executable code and treats user-defined plugins as callable functions. TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic. It also incorporates domain-specific knowledge through examples and ensures the secure execution of generated code. TaskWeaver offers a powerful and flexible framework for creating intelligent conversational agents that can handle complex tasks and adapt to domain-specific scenarios. The code is open-sourced at https://github.com/microsoft/TaskWeaver/.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Qiao, Bo and Li, Liqun and Zhang, Xu and He, Shilin and Kang, Yu and Zhang, Chaoyun and Yang, Fangkai and Dong, Hang and Zhang, Jue and Wang, Lu and Ma, Minghua and Zhao, Pu and Qin, Si and Qin, Xiaoting and Du, Chao and Xu, Yong and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei},
	month = dec,
	year = {2023},
	note = {arXiv:2311.17541 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\86TKLXR8\\Qiao et al. - 2023 - TaskWeaver A Code-First Agent Framework.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5X3LUSJN\\2311.html:text/html},
}

@misc{gao_strategyllm_2024,
	title = {{StrategyLLM}: {Large} {Language} {Models} as {Strategy} {Generators}, {Executors}, {Optimizers}, and {Evaluators} for {Problem} {Solving}},
	shorttitle = {{StrategyLLM}},
	url = {http://arxiv.org/abs/2311.08803},
	abstract = {Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21\% \${\textbackslash}rightarrow\$ 38.79\%), commonsense reasoning (70.3\% \${\textbackslash}rightarrow\$ 72.5\%), algorithmic reasoning (51.7\% \${\textbackslash}rightarrow\$ 62.0\%), and symbolic reasoning (30.0\% \${\textbackslash}rightarrow\$ 79.2\%).},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Gao, Chang and Jiang, Haiyun and Cai, Deng and Shi, Shuming and Lam, Wai},
	month = feb,
	year = {2024},
	note = {arXiv:2311.08803 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3K99ZU2M\\Gao et al. - 2024 - StrategyLLM Large Language Models as Strategy Gen.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\G9UGFRD5\\2311.html:text/html},
}

@misc{qi_large_2023,
	title = {Large {Language} {Models} are {Zero} {Shot} {Hypothesis} {Proposers}},
	url = {http://arxiv.org/abs/2311.05965},
	abstract = {Significant scientific discoveries have driven the progress of human civilisation. The explosion of scientific literature and data has created information barriers across disciplines that have slowed the pace of scientific discovery. Large Language Models (LLMs) hold a wealth of global and interdisciplinary knowledge that promises to break down these information barriers and foster a new wave of scientific discovery. However, the potential of LLMs for scientific discovery has not been formally explored. In this paper, we start from investigating whether LLMs can propose scientific hypotheses. To this end, we construct a dataset consist of background knowledge and hypothesis pairs from biomedical literature. The dataset is divided into training, seen, and unseen test sets based on the publication date to control visibility. We subsequently evaluate the hypothesis generation capabilities of various top-tier instructed models in zero-shot, few-shot, and fine-tuning settings, including both closed and open-source LLMs. Additionally, we introduce an LLM-based multi-agent cooperative framework with different role designs and external tools to enhance the capabilities related to generating hypotheses. We also design four metrics through a comprehensive review to evaluate the generated hypotheses for both ChatGPT-based and human evaluations. Through experiments and analyses, we arrive at the following findings: 1) LLMs surprisingly generate untrained yet validated hypotheses from testing literature. 2) Increasing uncertainty facilitates candidate generation, potentially enhancing zero-shot hypothesis generation capabilities. These findings strongly support the potential of LLMs as catalysts for new scientific discoveries and guide further exploration.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Qi, Biqing and Zhang, Kaiyan and Li, Haoxiang and Tian, Kai and Zeng, Sihang and Chen, Zhang-Ren and Zhou, Bowen},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05965 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AMXCULC9\\Qi et al. - 2023 - Large Language Models are Zero Shot Hypothesis Pro.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\N85H8JVD\\2311.html:text/html},
}

@misc{sun_interactive_2023,
	title = {Interactive {Planning} {Using} {Large} {Language} {Models} for {Partially} {Observable} {Robotics} {Tasks}},
	url = {http://arxiv.org/abs/2312.06876},
	abstract = {Designing robotic agents to perform open vocabulary tasks has been the long-standing goal in robotics and AI. Recently, Large Language Models (LLMs) have achieved impressive results in creating robotic agents for performing open vocabulary tasks. However, planning for these tasks in the presence of uncertainties is challenging as it requires {\textbackslash}enquote\{chain-of-thought\} reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates. In this paper, we present an interactive planning technique for partially observable tasks using LLMs. In the proposed method, an LLM is used to collect missing information from the environment using a robot and infer the state of the underlying problem from collected observations while guiding the robot to perform the required actions. We also use a fine-tuned Llama 2 model via self-instruct and compare its performance against a pre-trained LLM like GPT-4. Results are demonstrated on several tasks in simulation as well as real-world environments. A video describing our work along with some results could be found here.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Sun, Lingfeng and Jha, Devesh K. and Hori, Chiori and Jain, Siddarth and Corcodel, Radu and Zhu, Xinghao and Tomizuka, Masayoshi and Romeres, Diego},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TCKG3DAB\\Sun et al. - 2023 - Interactive Planning Using Large Language Models f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\U9JLXM5U\\2312.html:text/html},
}

@misc{zhuang_toolchain_2023,
	title = {{ToolChain}*: {Efficient} {Action} {Space} {Navigation} in {Large} {Language} {Models} with {A}* {Search}},
	shorttitle = {{ToolChain}*},
	url = {http://arxiv.org/abs/2310.13227},
	abstract = {Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A* search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1\% and 3.5\% on average while requiring 7.35x and 2.31x less time, respectively.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Zhuang, Yuchen and Chen, Xiang and Yu, Tong and Mitra, Saayan and Bursztyn, Victor and Rossi, Ryan A. and Sarkhel, Somdeb and Zhang, Chao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.13227 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IZBGXVAF\\Zhuang et al. - 2023 - ToolChain Efficient Action Space Navigation in L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AG6BSFKP\\2310.html:text/html},
}

@misc{liu_towards_2023,
	title = {Towards {Robust} {Multi}-{Modal} {Reasoning} via {Model} {Selection}},
	url = {http://arxiv.org/abs/2310.08446},
	abstract = {The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the \${\textbackslash}textit\{M\}{\textasciicircum}3\$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Liu, Xiangyan and Li, Rongxue and Ji, Wei and Lin, Tao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08446 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FF3BJ7PR\\Liu et al. - 2023 - Towards Robust Multi-Modal Reasoning via Model Sel.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\J9FSWXAH\\2310.html:text/html},
}

@misc{wang_recmind_2024,
	title = {{RecMind}: {Large} {Language} {Model} {Powered} {Agent} {For} {Recommendation}},
	shorttitle = {{RecMind}},
	url = {http://arxiv.org/abs/2308.14296},
	abstract = {While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Wang, Yancheng and Jiang, Ziyan and Chen, Zheng and Yang, Fan and Zhou, Yingxue and Cho, Eunah and Fan, Xing and Huang, Xiaojiang and Lu, Yanbin and Yang, Yingzhen},
	month = feb,
	year = {2024},
	note = {arXiv:2308.14296 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\F3EF6IYG\\Wang et al. - 2024 - RecMind Large Language Model Powered Agent For Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\KHFUMC2P\\2308.html:text/html},
}

@inproceedings{sun_adaplanner_2023-1,
	title = {{AdaPlanner}: {Adaptive} {Planning} from {Feedback} with {Language} {Models}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/b5c8c1c117618267944b2617add0a766-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao},
	editor = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {58202--58245},
}

@misc{wang_executable_2024,
	title = {Executable {Code} {Actions} {Elicit} {Better} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2402.01030},
	abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01030 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AJZVVGPA\\Wang et al. - 2024 - Executable Code Actions Elicit Better LLM Agents.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\NCQ68ILE\\2402.html:text/html},
}

@misc{zhu_knowagent_2024,
	title = {{KnowAgent}: {Knowledge}-{Augmented} {Planning} for {LLM}-{Based} {Agents}},
	shorttitle = {{KnowAgent}},
	url = {http://arxiv.org/abs/2403.03101},
	abstract = {Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Zhu, Yuqi and Qiao, Shuofei and Ou, Yixin and Deng, Shumin and Zhang, Ningyu and Lyu, Shiwei and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03101 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YVZFNIEG\\Zhu et al. - 2024 - KnowAgent Knowledge-Augmented Planning for LLM-Ba.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\V8RIE4ZG\\2403.html:text/html},
}

@misc{kim_prospector_2024,
	title = {Prospector: {Improving} {LLM} {Agents} with {Self}-{Asking} and {Trajectory} {Ranking}},
	url = {https://openreview.net/forum?id=YKK1jXEWja},
	author = {Kim, Byoungjip and Jang, Youngsoo and Logeswaran, Lajanugen and Kim, Geon-Hyeong and Kim, Yu Jin and Lee, Honglak and Lee, Moontae},
	year = {2024},
}

@misc{hirsch_whats_2024,
	title = {What's the {Plan}? {Evaluating} and {Developing} {Planning}-{Aware} {Techniques} for {LLMs}},
	shorttitle = {What's the {Plan}?},
	url = {http://arxiv.org/abs/2402.11489},
	abstract = {Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Hirsch, Eran and Uziel, Guy and Anaby-Tavor, Ateret},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11489 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\MJS4AA5X\\Hirsch et al. - 2024 - What's the Plan Evaluating and Developing Plannin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FVTP5XZF\\2402.html:text/html},
}

@misc{gao_efficient_2024,
	title = {Efficient {Tool} {Use} with {Chain}-of-{Abstraction} {Reasoning}},
	url = {http://arxiv.org/abs/2401.17464},
	abstract = {To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average {\textasciitilde}6\% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average {\textasciitilde}1.4x faster than baseline tool-augmented LLMs.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Gao, Silin and Dwivedi-Yu, Jane and Yu, Ping and Tan, Xiaoqing Ellen and Pasunuru, Ramakanth and Golovneva, Olga and Sinha, Koustuv and Celikyilmaz, Asli and Bosselut, Antoine and Wang, Tianlu},
	month = feb,
	year = {2024},
	note = {arXiv:2401.17464 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\HZGF6QRR\\Gao et al. - 2024 - Efficient Tool Use with Chain-of-Abstraction Reaso.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\MYKHLFRM\\2401.html:text/html},
}

@misc{li_more_2024,
	title = {More {Agents} {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2402.05120},
	abstract = {We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: {\textbackslash}url\{https://anonymous.4open.science/r/more\_agent\_is\_all\_you\_need\}.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05120 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WC4XXD3Q\\Li et al. - 2024 - More Agents Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9X74XWHI\\2402.html:text/html},
}

@misc{gao_efficient_2024-1,
	title = {Efficient {Tool} {Use} with {Chain}-of-{Abstraction} {Reasoning}},
	url = {http://arxiv.org/abs/2401.17464},
	abstract = {To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average {\textasciitilde}6\% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average {\textasciitilde}1.4x faster than baseline tool-augmented LLMs.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Gao, Silin and Dwivedi-Yu, Jane and Yu, Ping and Tan, Xiaoqing Ellen and Pasunuru, Ramakanth and Golovneva, Olga and Sinha, Koustuv and Celikyilmaz, Asli and Bosselut, Antoine and Wang, Tianlu},
	month = feb,
	year = {2024},
	note = {arXiv:2401.17464 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TPFG5I3T\\Gao et al. - 2024 - Efficient Tool Use with Chain-of-Abstraction Reaso.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\3VAX6G4M\\2401.html:text/html},
}

@inproceedings{ouyang_autoplan_2023-1,
	address = {Singapore},
	title = {{AutoPlan}: {Automatic} {Planning} of {Interactive} {Decision}-{Making} {Tasks} {With} {Large} {Language} {Models}},
	shorttitle = {{AutoPlan}},
	url = {https://aclanthology.org/2023.findings-emnlp.205},
	doi = {10.18653/v1/2023.findings-emnlp.205},
	language = {en},
	urldate = {2024-03-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ouyang, Siqi and Li, Lei},
	year = {2023},
	pages = {3114--3128},
	file = {Submitted Version:C\:\\Users\\ADobrovsky\\Zotero\\storage\\88L5DDVX\\Ouyang and Li - 2023 - AutoPlan Automatic Planning of Interactive Decisi.pdf:application/pdf},
}

@misc{aksitov_rest_2023,
	title = {{ReST} meets {ReAct}: {Self}-{Improvement} for {Multi}-{Step} {Reasoning} {LLM} {Agent}},
	shorttitle = {{ReST} meets {ReAct}},
	url = {http://arxiv.org/abs/2312.10003},
	abstract = {Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Aksitov, Renat and Miryoosefi, Sobhan and Li, Zonglin and Li, Daliang and Babayan, Sheila and Kopparapu, Kavya and Fisher, Zachary and Guo, Ruiqi and Prakash, Sushant and Srinivasan, Pranesh and Zaheer, Manzil and Yu, Felix and Kumar, Sanjiv},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10003 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\E8IIXEDR\\Aksitov et al. - 2023 - ReST meets ReAct Self-Improvement for Multi-Step .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\X5ICPUNT\\2312.html:text/html},
}

%%%%% Single Components




@misc{xie_translating_2023,
	title = {Translating {Natural} {Language} to {Planning} {Goals} with {Large}-{Language} {Models}},
	url = {http://arxiv.org/abs/2302.05128},
	abstract = {Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals specified in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domain-independent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TL5JAHZR\\Xie et al. - 2023 - Translating Natural Language to Planning Goals wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\I7P4XAQQ\\2302.html:text/html},
}

@article{shreyas_sundara_raman1_planning_nodate,
	title = {Planning with {Large} {Language} {Models} via {Corrective} {Re}-prompting},
	url = {https://openreview.net/pdf?id=cMDMRBe1TKs},
	author = {{Shreyas Sundara Raman∗,1} and {, Vanya Cohen2} and {, Eric Rosen1} and {, Ifrah Idrees1} and {,} and {David Paulius1 and Stefanie Tellex1}},
}

@misc{jin_impact_2024,
	title = {The {Impact} of {Reasoning} {Step} {Length} on {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.04925},
	abstract = {Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04925 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BCSCXQWY\\Jin et al. - 2024 - The Impact of Reasoning Step Length on Large Langu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5H2GQ64U\\2401.html:text/html},
}

@misc{creswell_faithful_2022,
	title = {Faithful {Reasoning} {Using} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2208.14271},
	abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Creswell, Antonia and Shanahan, Murray},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14271 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\B99NXUZI\\Creswell and Shanahan - 2022 - Faithful Reasoning Using Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PIV7EPBK\\2208.html:text/html},
}

@misc{zhou_language_2023,
	title = {Language {Agent} {Tree} {Search} {Unifies} {Reasoning} {Acting} and {Planning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2310.04406},
	abstract = {While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
	month = dec,
	year = {2023},
	note = {arXiv:2310.04406 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9D9D74KS\\Zhou et al. - 2023 - Language Agent Tree Search Unifies Reasoning Actin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\EP7K7BZ2\\2310.html:text/html},
}

@misc{yao_retroformer_2023,
	title = {Retroformer: {Retrospective} {Large} {Language} {Agents} with {Policy} {Gradient} {Optimization}},
	shorttitle = {Retroformer},
	url = {http://arxiv.org/abs/2308.02151},
	abstract = {Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Yao, Weiran and Heinecke, Shelby and Niebles, Juan Carlos and Liu, Zhiwei and Feng, Yihao and Xue, Le and Murthy, Rithesh and Chen, Zeyuan and Zhang, Jianguo and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
	month = aug,
	year = {2023},
	note = {arXiv:2308.02151 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GRJXHSNX\\Yao et al. - 2023 - Retroformer Retrospective Large Language Agents w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FLSZDMPU\\2308.html:text/html},
}

@article{liu_pre-train_2023,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {https://dl.acm.org/doi/10.1145/3560815},
	doi = {10.1145/3560815},
	abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input
	
	x
	
	and predict an output
	
	y
	
	as
	P
	(
	
	y{\textbar}x
	
	), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input
	
	x
	
	is modified using a
	template
	into a textual string
	prompt
	
	x′
	
	that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string
	
	x̂
	
	, from which the final output
	
	y
	
	can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be
	pre-trained
	on massive amounts of raw text, and by defining a new prompting function the model is able to perform
	few-shot
	or even
	zero-shot
	learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website
	NLPedia–Pretrain
	including constantly updated survey and paperlist.},
	language = {en},
	number = {9},
	urldate = {2024-02-03},
	journal = {ACM Computing Surveys},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = sep,
	year = {2023},
	pages = {1--35},
	file = {Submitted Version:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FSYVFZYN\\Liu et al. - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf:application/pdf},
}


@misc{press_measuring_2023,
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	month = oct,
	year = {2023},
	note = {arXiv:2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VA6H8CXX\\Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap i.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DHJWDKIE\\2210.html:text/html},
}


@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\K8WE48RQ\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IMTJ7EML\\2303.html:text/html},
}


@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BDSPXMN5\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JZX2PHPG\\2203.html:text/html},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GAQ47K9D\\Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6M4633W2\\2305.html:text/html},
}


@misc{zhang_large_2024,
	title = {Large {Language} {Models} as an {Indirect} {Reasoner}: {Contrapositive} and {Contradiction} for {Automated} {Reasoning}},
	shorttitle = {Large {Language} {Models} as an {Indirect} {Reasoner}},
	url = {http://arxiv.org/abs/2402.03667},
	abstract = {Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33\% and mathematical proof by 31.43\%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zhang, Yanfang and Sun, Yiliu and Zhan, Yibing and Tao, Dapeng and Tao, Dacheng and Gong, Chen},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03667 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\R5239D7S\\Zhang et al. - 2024 - Large Language Models as an Indirect Reasoner Con.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SE9MVA27\\2402.html:text/html},
}

@misc{srivastava_functional_2024,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ZVK6RSWF\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6TSFQN57\\2402.html:text/html},
}

@misc{long_large_2023,
	title = {Large {Language} {Model} {Guided} {Tree}-of-{Thought}},
	url = {http://arxiv.org/abs/2305.08291},
	abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: {\textbackslash}url\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Long, Jieyi},
	month = may,
	year = {2023},
	note = {arXiv:2305.08291 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PEII2F4N\\Long - 2023 - Large Language Model Guided Tree-of-Thought.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5E6HI2TT\\2305.html:text/html},
}

@misc{aksitov_rest_2023,
	title = {{ReST} meets {ReAct}: {Self}-{Improvement} for {Multi}-{Step} {Reasoning} {LLM} {Agent}},
	shorttitle = {{ReST} meets {ReAct}},
	url = {http://arxiv.org/abs/2312.10003},
	abstract = {Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Aksitov, Renat and Miryoosefi, Sobhan and Li, Zonglin and Li, Daliang and Babayan, Sheila and Kopparapu, Kavya and Fisher, Zachary and Guo, Ruiqi and Prakash, Sushant and Srinivasan, Pranesh and Zaheer, Manzil and Yu, Felix and Kumar, Sanjiv},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10003 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WJ7IFHBQ\\Aksitov et al. - 2023 - ReST meets ReAct Self-Improvement for Multi-Step .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IAS3R9YP\\2312.html:text/html},
}

@incollection{izquierdo-badiola_plancollabnl_2024,
	title = {{PlanCollabNL}: {Leveraging} {Large} {Language} {Models} for {Adaptive} {Plan} {Generation} in {Human}-{Robot} {Collaboration}},
	abstract = {"Hey, robot. Let's tidy up the kitchen. By the way, I have back pain today". How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models.},
	language = {English},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Izquierdo-Badiola, Silvia and Canal, Gerard and Rizzo, Carlos and Alenyà, Guillem},
	month = jan,
	year = {2024},
}

@misc{feng_large_2024,
	title = {Large {Language} {Model}-based {Human}-{Agent} {Collaboration} for {Complex} {Task} {Solving}},
	url = {http://arxiv.org/abs/2402.12914},
	abstract = {In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Feng, Xueyang and Chen, Zhi-Yuan and Qin, Yujia and Lin, Yankai and Chen, Xu and Liu, Zhiyuan and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.12914 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8JXQNAGR\\Feng et al. - 2024 - Large Language Model-based Human-Agent Collaborati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\CLKM9T4W\\2402.html:text/html},
}

@misc{fu_preact_2024,
	title = {{PreAct}: {Predicting} {Future} in {ReAct} {Enhances} {Agent}'s {Planning} {Ability}},
	shorttitle = {{PreAct}},
	url = {http://arxiv.org/abs/2402.11534},
	abstract = {Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce \${\textbackslash}textbf\{PreAct\}\$, an agent framework that integrates \${\textbackslash}textbf\{pre\}\$diction with \${\textbackslash}textbf\{rea\}\$soning and \${\textbackslash}textbf\{act\}\$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Fu, Dayuan and Huang, Jianzhao and Lu, Siyuan and Dong, Guanting and Wang, Yejie and He, Keqing and Xu, Weiran},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11534 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FTXJZS6H\\Fu et al. - 2024 - PreAct Predicting Future in ReAct Enhances Agent'.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ALGCC9GJ\\2402.html:text/html},
}

@misc{chen_when_2024,
	title = {When is {Tree} {Search} {Useful} for {LLM} {Planning}? {It} {Depends} on the {Discriminator}},
	shorttitle = {When is {Tree} {Search} {Useful} for {LLM} {Planning}?},
	url = {http://arxiv.org/abs/2402.10890},
	abstract = {In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90\% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Chen, Ziru and White, Michael and Mooney, Raymond and Payani, Ali and Su, Yu and Sun, Huan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10890 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IS3DGRWU\\Chen et al. - 2024 - When is Tree Search Useful for LLM Planning It De.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\KR4NJ6PU\\2402.html:text/html},
}

@misc{yuan_easytool_2024,
	title = {{EASYTOOL}: {Enhancing} {LLM}-based {Agents} with {Concise} {Tool} {Instruction}},
	shorttitle = {{EASYTOOL}},
	url = {http://arxiv.org/abs/2401.06201},
	abstract = {To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at {\textbackslash}url\{https://github.com/microsoft/JARVIS/\} in the future.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Yuan, Siyu and Song, Kaitao and Chen, Jiangjie and Tan, Xu and Shen, Yongliang and Kan, Ren and Li, Dongsheng and Yang, Deqing},
	month = feb,
	year = {2024},
	note = {arXiv:2401.06201 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QPR2CMRB\\Yuan et al. - 2024 - EASYTOOL Enhancing LLM-based Agents with Concise .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ECY3656D\\2401.html:text/html},
}


@misc{wang_rat_2024,
	title = {{RAT}: {Retrieval} {Augmented} {Thoughts} {Elicit} {Context}-{Aware} {Reasoning} in {Long}-{Horizon} {Generation}},
	shorttitle = {{RAT}},
	url = {http://arxiv.org/abs/2403.05313},
	abstract = {We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63\% on code generation, 16.96\% on mathematical reasoning, 19.2\% on creative writing, and 42.78\% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Wang, Zihao and Liu, Anji and Lin, Haowei and Li, Jiaqi and Ma, Xiaojian and Liang, Yitao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05313 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DIGQWZYT\\Wang et al. - 2024 - RAT Retrieval Augmented Thoughts Elicit Context-A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6W3PS4AW\\2403.html:text/html},
}


@misc{kagaya_rap_2024,
	title = {{RAP}: {Retrieval}-{Augmented} {Planning} with {Contextual} {Memory} for {Multimodal} {LLM} {Agents}},
	shorttitle = {{RAP}},
	url = {http://arxiv.org/abs/2402.03610},
	abstract = {Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Kagaya, Tomoyuki and Yuan, Thong Jing and Lou, Yuxuan and Karlekar, Jayashree and Pranata, Sugiri and Kinose, Akira and Oguri, Koki and Wick, Felix and You, Yang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03610 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\34CBKTRY\\Kagaya et al. - 2024 - RAP Retrieval-Augmented Planning with Contextual .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\2NXSIEMH\\2402.html:text/html},
}

@misc{guo_empowering_2023,
	title = {Empowering {Working} {Memory} for {Large} {Language} {Model} {Agents}},
	url = {http://arxiv.org/abs/2312.17259},
	abstract = {Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Guo, Jing and Li, Nan and Qi, Jianchuan and Yang, Hang and Li, Ruiqiao and Feng, Yuzhen and Zhang, Si and Xu, Ming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17259 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\775EUZ82\\Guo et al. - 2023 - Empowering Working Memory for Large Language Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ZUVWVMQ5\\2312.html:text/html},
}

@misc{srivastava_functional_2024-1,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VGNBI32K\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\CVXPJALE\\2402.html:text/html},
}


@misc{karpas_mrkl_2022,
	title = {{MRKL} {Systems}: {A} modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
	shorttitle = {{MRKL} {Systems}},
	url = {http://arxiv.org/abs/2205.00445},
	abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
	month = may,
	year = {2022},
	note = {arXiv:2205.00445 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\28NKST66\\Karpas et al. - 2022 - MRKL Systems A modular, neuro-symbolic architectu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\Q883LZHR\\2205.html:text/html},
}


%%%% Evaluations 


@misc{guo_pptc_2023,
	title = {{PPTC} {Benchmark}: {Evaluating} {Large} {Language} {Models} for {PowerPoint} {Task} {Completion}},
	shorttitle = {{PPTC} {Benchmark}},
	url = {http://arxiv.org/abs/2311.01767},
	abstract = {Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1{\textbackslash}\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6{\textbackslash}\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at {\textbackslash}url\{https://github.com/gydpku/PPTC\}.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Guo, Yiduo and Zhang, Zekai and Liang, Yaobo and Zhao, Dongyan and Duan, Nan},
	month = nov,
	year = {2023},
	note = {arXiv:2311.01767 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PL5UPGUU\\Guo et al. - 2023 - PPTC Benchmark Evaluating Large Language Models f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FPICTWUT\\2311.html:text/html},
}

@misc{mialon_gaia_2023,
	title = {{GAIA}: a benchmark for {General} {AI} {Assistants}},
	shorttitle = {{GAIA}},
	url = {http://arxiv.org/abs/2311.12983},
	abstract = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92{\textbackslash}\% vs. 15{\textbackslash}\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12983 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FQN8YUN7\\Mialon et al. - 2023 - GAIA a benchmark for General AI Assistants.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\7DIQUKVP\\2311.html:text/html},
}

@misc{valmeekam_planning_2023,
	title = {On the {Planning} {Abilities} of {Large} {Language} {Models} ({A} {Critical} {Investigation} with a {Proposed} {Benchmark})},
	url = {http://arxiv.org/abs/2302.06706},
	abstract = {Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3\% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Sreedharan, Sarath and Marquez, Matthew and Olmo, Alberto and Kambhampati, Subbarao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06706 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\36MV6XC2\\Valmeekam et al. - 2023 - On the Planning Abilities of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BRUCQPCM\\2302.html:text/html},
}

@misc{wu_smartplay_2023,
	title = {{SmartPlay}: {A} {Benchmark} for {LLMs} as {Intelligent} {Agents}},
	shorttitle = {{SmartPlay}},
	url = {http://arxiv.org/abs/2310.01557},
	abstract = {Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/microsoft/SmartPlay},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Wu, Yue and Tang, Xuan and Mitchell, Tom M. and Li, Yuanzhi},
	month = dec,
	year = {2023},
	note = {arXiv:2310.01557 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DNR4BZJ7\\Wu et al. - 2023 - SmartPlay A Benchmark for LLMs as Intelligent Age.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BARCPJNH\\2310.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4IH29PEM\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\96B3TUA5\\2304.html:text/html},
}

@misc{valmeekam_planbench_2023,
	title = {{PlanBench}: {An} {Extensible} {Benchmark} for {Evaluating} {Large} {Language} {Models} on {Planning} and {Reasoning} about {Change}},
	shorttitle = {{PlanBench}},
	url = {http://arxiv.org/abs/2206.10498},
	abstract = {Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2206.10498 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GZA6HZFG\\Valmeekam et al. - 2023 - PlanBench An Extensible Benchmark for Evaluating .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\T223HCI2\\2206.html:text/html},
}

@misc{yang_planning_2023,
	title = {On the {Planning}, {Search}, and {Memorization} {Capabilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.01868},
	abstract = {The rapid advancement of large language models, such as the Generative Pre-trained Transformer (GPT) series, has had significant implications across various disciplines. In this study, we investigate the potential of the state-of-the-art large language model (GPT-4) for planning tasks. We explore its effectiveness in multiple planning subfields, highlighting both its strengths and limitations. Through a comprehensive examination, we identify areas where large language models excel in solving planning problems and reveal the constraints that limit their applicability. Our empirical analysis focuses on GPT-4's performance in planning domain extraction, graph search path planning, and adversarial planning. We then propose a way of fine-tuning a domain-specific large language model to improve its Chain of Thought (CoT) capabilities for the above-mentioned tasks. The results provide valuable insights into the potential applications of large language models in the planning domain and pave the way for future research to overcome their limitations and expand their capabilities.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Yang, Yunhao and Tomar, Anshul},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01868 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BLQTZMLK\\Yang and Tomar - 2023 - On the Planning, Search, and Memorization Capabili.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\F6W5GL8X\\2309.html:text/html},
}

@misc{pallagani_understanding_2023,
	title = {Understanding the {Capabilities} of {Large} {Language} {Models} for {Automated} {Planning}},
	url = {http://arxiv.org/abs/2305.16151},
	abstract = {Automated planning is concerned with developing efficient algorithms to generate plans or sequences of actions to achieve a specific goal in a given environment. Emerging Large Language Models (LLMs) can answer questions, write high-quality programming code, and predict protein folding, showcasing their versatility in solving various tasks beyond language-based problems. In this paper, we aim to explore how LLMs can also be used for automated planning. To do so, we seek to answer four key questions. Firstly, we want to understand the extent to which LLMs can be used for plan generation. Secondly, we aim to identify which pre-training data is most effective in facilitating plan generation. Thirdly, we investigate whether fine-tuning or prompting is a more effective approach for plan generation. Finally, we explore whether LLMs are capable of plan generalization. By answering these questions, the study seeks to shed light on the capabilities of LLMs in solving complex planning problems and provide insights into the most effective approaches for using LLMs in this context.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Srivastava, Biplav and Horesh, Lior and Fabiano, Francesco and Loreggia, Andrea},
	month = may,
	year = {2023},
	note = {arXiv:2305.16151 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ELVBCPWW\\Pallagani et al. - 2023 - Understanding the Capabilities of Large Language M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6P982XTH\\2305.html:text/html},
}

@incollection{hammer_evaluation_2023,
	address = {Cham},
	title = {Evaluation of {Pretrained} {Large} {Language} {Models} in {Embodied} {Planning} {Tasks}},
	volume = {13921},
	isbn = {978-3-031-33468-9 978-3-031-33469-6},
	url = {https://link.springer.com/10.1007/978-3-031-33469-6_23},
	language = {en},
	urldate = {2024-02-01},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Sarkisyan, Christina and Korchemnyi, Alexandr and Kovalev, Alexey K. and Panov, Aleksandr I.},
	editor = {Hammer, Patrick and Alirezaie, Marjan and Strannegård, Claes},
	year = {2023},
	doi = {10.1007/978-3-031-33469-6_23},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {222--232},
}

@misc{ma_agentboard_2024,
	title = {{AgentBoard}: {An} {Analytical} {Evaluation} {Board} of {Multi}-turn {LLM} {Agents}},
	shorttitle = {{AgentBoard}},
	url = {http://arxiv.org/abs/2401.13178},
	abstract = {Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Ma, Chang and Zhang, Junlei and Zhu, Zhihao and Yang, Cheng and Yang, Yujiu and Jin, Yaohui and Lan, Zhenzhong and Kong, Lingpeng and He, Junxian},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13178 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SFKV796R\\Ma et al. - 2024 - AgentBoard An Analytical Evaluation Board of Mult.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QKCVHS23\\2401.html:text/html},
}

@misc{furuta_language_2023,
	title = {Language {Model} {Agents} {Suffer} from {Compositional} {Generalization} in {Web} {Automation}},
	url = {http://arxiv.org/abs/2311.18751},
	abstract = {Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0\% average success rate on base tasks, their performance degrades to 24.9\% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4\% to 54.8\%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2\%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5\%). While these highlight the promise of small-scale finetuned and transferred models for compositional generalization, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Furuta, Hiroki and Matsuo, Yutaka and Faust, Aleksandra and Gur, Izzeddin},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18751 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QGQ9U2PX\\Furuta et al. - 2023 - Language Model Agents Suffer from Compositional Ge.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5JFVHANV\\2311.html:text/html},
}



@incollection{hammer_can_2023,
	address = {Cham},
	title = {Can {Language} {Models} {Be} {Used} in {Multistep} {Commonsense} {Planning} {Domains}?},
	volume = {13921},
	isbn = {978-3-031-33468-9 978-3-031-33469-6},
	url = {https://link.springer.com/10.1007/978-3-031-33469-6_28},
	language = {en},
	urldate = {2024-02-03},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Tang, Zhisheng and Kejriwal, Mayank},
	editor = {Hammer, Patrick and Alirezaie, Marjan and Strannegård, Claes},
	year = {2023},
	doi = {10.1007/978-3-031-33469-6_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {276--285},
}

@misc{liu_bolaa_2023-1,
	title = {{BOLAA}: {Benchmarking} and {Orchestrating} {LLM}-augmented {Autonomous} {Agents}},
	shorttitle = {{BOLAA}},
	url = {http://arxiv.org/abs/2308.05960},
	abstract = {The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, {\textbackslash}textit\{i.e.\} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at {\textbackslash}url\{https://github.com/salesforce/BOLAA\}.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05960 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Preprint},
}

@misc{nan_evaluating_2023,
	title = {On {Evaluating} the {Integration} of {Reasoning} and {Action} in {LLM} {Agents} with {Database} {Question} {Answering}},
	url = {http://arxiv.org/abs/2311.09721},
	abstract = {This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Nan, Linyong and Zhang, Ellen and Zou, Weijin and Zhao, Yilun and Zhou, Wenfei and Cohan, Arman},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09721 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WUBAB6SQ\\Nan et al. - 2023 - On Evaluating the Integration of Reasoning and Act.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TBAM8J3N\\2311.html:text/html},
}

@inproceedings{valmeekam_planbench_2023-1,
	title = {{PlanBench}: {An} {Extensible} {Benchmark} for {Evaluating} {Large} {Language} {Models} on {Planning} and {Reasoning} about {Change}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7a92bcdede88c7afd108072faf5485c8-Paper-Datasets_and_Benchmarks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
	editor = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {38975--38987},
}

@inproceedings{li_api-bank_2023,
	address = {Singapore},
	title = {{API}-{Bank}: {A} {Comprehensive} {Benchmark} for {Tool}-{Augmented} {LLMs}},
	shorttitle = {{API}-{Bank}},
	url = {https://aclanthology.org/2023.emnlp-main.187},
	doi = {10.18653/v1/2023.emnlp-main.187},
	language = {en},
	urldate = {2024-03-13},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
	year = {2023},
	pages = {3102--3116},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PAWQUHLD\\Li et al. - 2023 - API-Bank A Comprehensive Benchmark for Tool-Augme.pdf:application/pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%Single Components

@misc{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {http://arxiv.org/abs/2201.07207},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = mar,
	year = {2022},
	note = {arXiv:2201.07207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, INCLUDED},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UPQKMVUY\\Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IZUQVLAQ\\2201.html:text/html},
}

@misc{tang_towards_2023,
	title = {Towards {CausalGPT}: {A} {Multi}-{Agent} {Approach} for {Faithful} {Knowledge} {Reasoning} via {Promoting} {Causal} {Consistency} in {LLMs}},
	shorttitle = {Towards {CausalGPT}},
	url = {http://arxiv.org/abs/2308.11914},
	abstract = {Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the {\textbackslash}textit\{evaluator\} agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Tang, Ziyi and Wang, Ruilin and Chen, Weixing and Wang, Keze and Liu, Yang and Chen, Tianshui and Lin, Liang},
	month = sep,
	year = {2023},
	note = {arXiv:2308.11914 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, INCLUDED},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\UBEH92I6\\Tang et al. - 2023 - Towards CausalGPT A Multi-Agent Approach for Fait.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SI8AGECH\\2308.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4IH29PEM\\Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\96B3TUA5\\2304.html:text/html},
}

@misc{chia_contrastive_2023,
	title = {Contrastive {Chain}-of-{Thought} {Prompting}},
	url = {http://arxiv.org/abs/2311.09277},
	abstract = {Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Chia, Yew Ken and Chen, Guizhen and Tuan, Luu Anh and Poria, Soujanya and Bing, Lidong},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09277 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5Y3CDT8N\\Chia et al. - 2023 - Contrastive Chain-of-Thought Prompting.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\G6C3H895\\2311.html:text/html},
}

@misc{zhang_igniting_2023,
	title = {Igniting {Language} {Intelligence}: {The} {Hitchhiker}'s {Guide} {From} {Chain}-of-{Thought} {Reasoning} to {Language} {Agents}},
	shorttitle = {Igniting {Language} {Intelligence}},
	url = {http://arxiv.org/abs/2311.11797},
	abstract = {Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Yao, Yao and Zhang, Aston and Tang, Xiangru and Ma, Xinbei and He, Zhiwei and Wang, Yiming and Gerstein, Mark and Wang, Rui and Liu, Gongshen and Zhao, Hai},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11797 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multiagent Systems, Computer Science - Human-Computer Interaction, Preprint},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\Y3EDZ8G7\\Zhang et al. - 2023 - Igniting Language Intelligence The Hitchhiker's G.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\YXBLNYT7\\2311.html:text/html},
}

@misc{xie_translating_2023,
	title = {Translating {Natural} {Language} to {Planning} {Goals} with {Large}-{Language} {Models}},
	url = {http://arxiv.org/abs/2302.05128},
	abstract = {Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals specified in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domain-independent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Xie, Yaqi and Yu, Chen and Zhu, Tongyao and Bai, Jinbin and Gong, Ze and Soh, Harold},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05128 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\TL5JAHZR\\Xie et al. - 2023 - Translating Natural Language to Planning Goals wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\I7P4XAQQ\\2302.html:text/html},
}

@article{shreyas_sundara_raman1_planning_nodate,
	title = {Planning with {Large} {Language} {Models} via {Corrective} {Re}-prompting},
	url = {https://openreview.net/pdf?id=cMDMRBe1TKs},
	author = {{Shreyas Sundara Raman∗,1} and {, Vanya Cohen2} and {, Eric Rosen1} and {, Ifrah Idrees1} and {,} and {David Paulius1 and Stefanie Tellex1}},
}

@misc{jin_impact_2024,
	title = {The {Impact} of {Reasoning} {Step} {Length} on {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.04925},
	abstract = {Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04925 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BCSCXQWY\\Jin et al. - 2024 - The Impact of Reasoning Step Length on Large Langu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5H2GQ64U\\2401.html:text/html},
}

@misc{creswell_faithful_2022,
	title = {Faithful {Reasoning} {Using} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2208.14271},
	abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Creswell, Antonia and Shanahan, Murray},
	month = aug,
	year = {2022},
	note = {arXiv:2208.14271 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\B99NXUZI\\Creswell and Shanahan - 2022 - Faithful Reasoning Using Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PIV7EPBK\\2208.html:text/html},
}

@misc{zhou_language_2023,
	title = {Language {Agent} {Tree} {Search} {Unifies} {Reasoning} {Acting} and {Planning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2310.04406},
	abstract = {While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
	month = dec,
	year = {2023},
	note = {arXiv:2310.04406 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9D9D74KS\\Zhou et al. - 2023 - Language Agent Tree Search Unifies Reasoning Actin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\EP7K7BZ2\\2310.html:text/html},
}

@misc{song_llm-planner_2023,
	title = {{LLM}-{Planner}: {Few}-{Shot} {Grounded} {Planning} for {Embodied} {Agents} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Planner}},
	url = {http://arxiv.org/abs/2212.04088},
	abstract = {This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5\% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2212.04088 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, INCLUDED},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SP935CTA\\Song et al. - 2023 - LLM-Planner Few-Shot Grounded Planning for Embodi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ANNCAP28\\2212.html:text/html},
}

@misc{yao_retroformer_2023,
	title = {Retroformer: {Retrospective} {Large} {Language} {Agents} with {Policy} {Gradient} {Optimization}},
	shorttitle = {Retroformer},
	url = {http://arxiv.org/abs/2308.02151},
	abstract = {Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Yao, Weiran and Heinecke, Shelby and Niebles, Juan Carlos and Liu, Zhiwei and Feng, Yihao and Xue, Le and Murthy, Rithesh and Chen, Zeyuan and Zhang, Jianguo and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
	month = aug,
	year = {2023},
	note = {arXiv:2308.02151 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GRJXHSNX\\Yao et al. - 2023 - Retroformer Retrospective Large Language Agents w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FLSZDMPU\\2308.html:text/html},
}

@article{liu_pre-train_2023,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {https://dl.acm.org/doi/10.1145/3560815},
	doi = {10.1145/3560815},
	abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input
	
	x
	
	and predict an output
	
	y
	
	as
	P
	(
	
	y{\textbar}x
	
	), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input
	
	x
	
	is modified using a
	template
	into a textual string
	prompt
	
	x′
	
	that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string
	
	x̂
	
	, from which the final output
	
	y
	
	can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be
	pre-trained
	on massive amounts of raw text, and by defining a new prompting function the model is able to perform
	few-shot
	or even
	zero-shot
	learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website
	NLPedia–Pretrain
	including constantly updated survey and paperlist.},
	language = {en},
	number = {9},
	urldate = {2024-02-03},
	journal = {ACM Computing Surveys},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = sep,
	year = {2023},
	pages = {1--35},
	file = {Submitted Version:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FSYVFZYN\\Liu et al. - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf:application/pdf},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\HD5E3UJP\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\C7LKF8TF\\2205.html:text/html},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-{Solve} {Prompting}: {Improving} {Zero}-{Shot} {Chain}-of-{Thought} {Reasoning} by {Large} {Language} {Models}},
	shorttitle = {Plan-and-{Solve} {Prompting}},
	url = {http://arxiv.org/abs/2305.04091},
	abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	month = may,
	year = {2023},
	note = {arXiv:2305.04091 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BNVLUDU3\\Wang et al. - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chai.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9H4SNWX9\\2305.html:text/html},
}

@misc{press_measuring_2023,
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	month = oct,
	year = {2023},
	note = {arXiv:2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VA6H8CXX\\Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap i.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DHJWDKIE\\2210.html:text/html},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\J2BCFXCJ\\Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\4ZQ3W4DT\\2210.html:text/html},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\K8WE48RQ\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IMTJ7EML\\2303.html:text/html},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\AD3GKQ59\\Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforceme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IMBA9DI5\\2303.html:text/html},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\BDSPXMN5\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\JZX2PHPG\\2203.html:text/html},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = dec,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\GAQ47K9D\\Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6M4633W2\\2305.html:text/html},
}

@misc{besta_graph_2023,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = nov,
	year = {2023},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\889WHWFQ\\Besta et al. - 2023 - Graph of Thoughts Solving Elaborate Problems with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QX8ABTJ8\\2308.html:text/html},
}

@misc{zhang_large_2024,
	title = {Large {Language} {Models} as an {Indirect} {Reasoner}: {Contrapositive} and {Contradiction} for {Automated} {Reasoning}},
	shorttitle = {Large {Language} {Models} as an {Indirect} {Reasoner}},
	url = {http://arxiv.org/abs/2402.03667},
	abstract = {Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33\% and mathematical proof by 31.43\%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Zhang, Yanfang and Sun, Yiliu and Zhan, Yibing and Tao, Dapeng and Tao, Dacheng and Gong, Chen},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03667 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\R5239D7S\\Zhang et al. - 2024 - Large Language Models as an Indirect Reasoner Con.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\SE9MVA27\\2402.html:text/html},
}

@misc{srivastava_functional_2024,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ZVK6RSWF\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6TSFQN57\\2402.html:text/html},
}

@misc{liu_towards_2023,
	title = {Towards {Robust} {Multi}-{Modal} {Reasoning} via {Model} {Selection}},
	url = {http://arxiv.org/abs/2310.08446},
	abstract = {The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the \${\textbackslash}textit\{M\}{\textasciicircum}3\$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Liu, Xiangyan and Li, Rongxue and Ji, Wei and Lin, Tao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FF3BJ7PR\\Liu et al. - 2023 - Towards Robust Multi-Modal Reasoning via Model Sel.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\J9FSWXAH\\2310.html:text/html},
}

@misc{long_large_2023,
	title = {Large {Language} {Model} {Guided} {Tree}-of-{Thought}},
	url = {http://arxiv.org/abs/2305.08291},
	abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: {\textbackslash}url\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Long, Jieyi},
	month = may,
	year = {2023},
	note = {arXiv:2305.08291 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\PEII2F4N\\Long - 2023 - Large Language Model Guided Tree-of-Thought.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\5E6HI2TT\\2305.html:text/html},
}

@misc{aksitov_rest_2023,
	title = {{ReST} meets {ReAct}: {Self}-{Improvement} for {Multi}-{Step} {Reasoning} {LLM} {Agent}},
	shorttitle = {{ReST} meets {ReAct}},
	url = {http://arxiv.org/abs/2312.10003},
	abstract = {Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Aksitov, Renat and Miryoosefi, Sobhan and Li, Zonglin and Li, Daliang and Babayan, Sheila and Kopparapu, Kavya and Fisher, Zachary and Guo, Ruiqi and Prakash, Sushant and Srinivasan, Pranesh and Zaheer, Manzil and Yu, Felix and Kumar, Sanjiv},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10003 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WJ7IFHBQ\\Aksitov et al. - 2023 - ReST meets ReAct Self-Improvement for Multi-Step .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IAS3R9YP\\2312.html:text/html},
}

@incollection{izquierdo-badiola_plancollabnl_2024,
	title = {{PlanCollabNL}: {Leveraging} {Large} {Language} {Models} for {Adaptive} {Plan} {Generation} in {Human}-{Robot} {Collaboration}},
	abstract = {"Hey, robot. Let's tidy up the kitchen. By the way, I have back pain today". How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models.},
	language = {English},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Izquierdo-Badiola, Silvia and Canal, Gerard and Rizzo, Carlos and Alenyà, Guillem},
	month = jan,
	year = {2024},
}

@misc{feng_large_2024,
	title = {Large {Language} {Model}-based {Human}-{Agent} {Collaboration} for {Complex} {Task} {Solving}},
	url = {http://arxiv.org/abs/2402.12914},
	abstract = {In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Feng, Xueyang and Chen, Zhi-Yuan and Qin, Yujia and Lin, Yankai and Chen, Xu and Liu, Zhiyuan and Wen, Ji-Rong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.12914 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8JXQNAGR\\Feng et al. - 2024 - Large Language Model-based Human-Agent Collaborati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\CLKM9T4W\\2402.html:text/html},
}

@misc{fu_preact_2024,
	title = {{PreAct}: {Predicting} {Future} in {ReAct} {Enhances} {Agent}'s {Planning} {Ability}},
	shorttitle = {{PreAct}},
	url = {http://arxiv.org/abs/2402.11534},
	abstract = {Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce \${\textbackslash}textbf\{PreAct\}\$, an agent framework that integrates \${\textbackslash}textbf\{pre\}\$diction with \${\textbackslash}textbf\{rea\}\$soning and \${\textbackslash}textbf\{act\}\$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Fu, Dayuan and Huang, Jianzhao and Lu, Siyuan and Dong, Guanting and Wang, Yejie and He, Keqing and Xu, Weiran},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11534 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FTXJZS6H\\Fu et al. - 2024 - PreAct Predicting Future in ReAct Enhances Agent'.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ALGCC9GJ\\2402.html:text/html},
}

@misc{chen_when_2024,
	title = {When is {Tree} {Search} {Useful} for {LLM} {Planning}? {It} {Depends} on the {Discriminator}},
	shorttitle = {When is {Tree} {Search} {Useful} for {LLM} {Planning}?},
	url = {http://arxiv.org/abs/2402.10890},
	abstract = {In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90\% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Chen, Ziru and White, Michael and Mooney, Raymond and Payani, Ali and Su, Yu and Sun, Huan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10890 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\IS3DGRWU\\Chen et al. - 2024 - When is Tree Search Useful for LLM Planning It De.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\KR4NJ6PU\\2402.html:text/html},
}

@misc{yuan_easytool_2024,
	title = {{EASYTOOL}: {Enhancing} {LLM}-based {Agents} with {Concise} {Tool} {Instruction}},
	shorttitle = {{EASYTOOL}},
	url = {http://arxiv.org/abs/2401.06201},
	abstract = {To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at {\textbackslash}url\{https://github.com/microsoft/JARVIS/\} in the future.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Yuan, Siyu and Song, Kaitao and Chen, Jiangjie and Tan, Xu and Shen, Yongliang and Kan, Ren and Li, Dongsheng and Yang, Deqing},
	month = feb,
	year = {2024},
	note = {arXiv:2401.06201 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\QPR2CMRB\\Yuan et al. - 2024 - EASYTOOL Enhancing LLM-based Agents with Concise .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ECY3656D\\2401.html:text/html},
}

@misc{kagaya_rap_2024,
	title = {{RAP}: {Retrieval}-{Augmented} {Planning} with {Contextual} {Memory} for {Multimodal} {LLM} {Agents}},
	shorttitle = {{RAP}},
	url = {http://arxiv.org/abs/2402.03610},
	abstract = {Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Kagaya, Tomoyuki and Yuan, Thong Jing and Lou, Yuxuan and Karlekar, Jayashree and Pranata, Sugiri and Kinose, Akira and Oguri, Koki and Wick, Felix and You, Yang},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03610 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\34CBKTRY\\Kagaya et al. - 2024 - RAP Retrieval-Augmented Planning with Contextual .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\2NXSIEMH\\2402.html:text/html},
}

@misc{guo_empowering_2023,
	title = {Empowering {Working} {Memory} for {Large} {Language} {Model} {Agents}},
	url = {http://arxiv.org/abs/2312.17259},
	abstract = {Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Guo, Jing and Li, Nan and Qi, Jianchuan and Yang, Hang and Li, Ruiqiao and Feng, Yuzhen and Zhang, Si and Xu, Ming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17259 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\775EUZ82\\Guo et al. - 2023 - Empowering Working Memory for Large Language Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\ZUVWVMQ5\\2312.html:text/html},
}

@misc{srivastava_functional_2024-1,
	title = {Functional {Benchmarks} for {Robust} {Evaluation} of {Reasoning} {Performance}, and the {Reasoning} {Gap}},
	url = {http://arxiv.org/abs/2402.19450},
	abstract = {We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35\% to 80.31\% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Srivastava, Saurabh and B, Annarose M. and P V, Anto and Menon, Shashank and Sukumar, Ajay and T, Adwaith Samod and Philipose, Alan and Prince, Stevin and Thomas, Sooraj},
	month = feb,
	year = {2024},
	note = {arXiv:2402.19450 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\VGNBI32K\\Srivastava et al. - 2024 - Functional Benchmarks for Robust Evaluation of Rea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\CVXPJALE\\2402.html:text/html},
}

@misc{sel_algorithm_2023,
	title = {Algorithm of {Thoughts}: {Enhancing} {Exploration} of {Ideas} in {Large} {Language} {Models}},
	shorttitle = {Algorithm of {Thoughts}},
	url = {http://arxiv.org/abs/2308.10379},
	abstract = {Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Jia, Ruoxi and Jin, Ming},
	month = sep,
	year = {2023},
	note = {arXiv:2308.10379 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\9FJJ69K9\\Sel et al. - 2023 - Algorithm of Thoughts Enhancing Exploration of Id.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\WLV5JXKT\\2308.html:text/html},
}

@misc{wang_rat_2024,
	title = {{RAT}: {Retrieval} {Augmented} {Thoughts} {Elicit} {Context}-{Aware} {Reasoning} in {Long}-{Horizon} {Generation}},
	shorttitle = {{RAT}},
	url = {http://arxiv.org/abs/2403.05313},
	abstract = {We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63\% on code generation, 16.96\% on mathematical reasoning, 19.2\% on creative writing, and 42.78\% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Wang, Zihao and Liu, Anji and Lin, Haowei and Li, Jiaqi and Ma, Xiaojian and Liang, Yitao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05313 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, INCLUDED},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\DIGQWZYT\\Wang et al. - 2024 - RAT Retrieval Augmented Thoughts Elicit Context-A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\6W3PS4AW\\2403.html:text/html},
}


@misc{gou_critic_2024,
	title = {{CRITIC}: {Large} {Language} {Models} {Can} {Self}-{Correct} with {Tool}-{Interactive} {Critiquing}},
	shorttitle = {{CRITIC}},
	url = {http://arxiv.org/abs/2305.11738},
	abstract = {Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
	month = feb,
	year = {2024},
	note = {arXiv:2305.11738 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\2MTBQB6D\\Gou et al. - 2024 - CRITIC Large Language Models Can Self-Correct wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\8FBH8ZZR\\2305.html:text/html},
}


@misc{miao_selfcheck_2023,
	title = {{SelfCheck}: {Using} {LLMs} to {Zero}-{Shot} {Check} {Their} {Own} {Step}-by-{Step} {Reasoning}},
	shorttitle = {{SelfCheck}},
	url = {http://arxiv.org/abs/2308.00436},
	abstract = {The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Miao, Ning and Teh, Yee Whye and Rainforth, Tom},
	month = oct,
	year = {2023},
	note = {arXiv:2308.00436 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ADobrovsky\\Zotero\\storage\\FMVPA7FT\\Miao et al. - 2023 - SelfCheck Using LLMs to Zero-Shot Check Their Own.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ADobrovsky\\Zotero\\storage\\LZZZ3HF5\\2308.html:text/html},
}


@inproceedings{chen_chatcot_2023,
	address = {Singapore},
	title = {{ChatCoT}: {Tool}-{Augmented} {Chain}-of-{Thought} {Reasoning} on {Chat}-based {Large} {Language} {Models}},
	shorttitle = {{ChatCoT}},
	url = {https://aclanthology.org/2023.findings-emnlp.985},
	doi = {10.18653/v1/2023.findings-emnlp.985},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Zhipeng and Zhou, Kun and Zhang, Beichen and Gong, Zheng and Zhao, Xin and Wen, Ji-Rong},
	year = {2023},
	pages = {14777--14790},
	file = {Full Text:C\:\\Users\\ADobrovsky\\Zotero\\storage\\LNUTIKR9\\Chen et al. - 2023 - ChatCoT Tool-Augmented Chain-of-Thought Reasoning.pdf:application/pdf},
}
